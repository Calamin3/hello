{
  
    
        "post0": {
            "title": "MRT OD Matrix in Singapore",
            "content": "Introduction . In Singapore, one of the main forms of public transportation is via MRT/LRT. One assignment that I have performed is to use the commuting data for MRT and employ appropriate cluster analysis methods to group these stations into homogeneous groups using the origin-destination data. We are interested to find out the weekday AM (7-9am) and PM peak (5-7pm) for the journey-to-work and journey-to-home trips, and for weekend interaction during noon times (11-1pm). While the assignment was performed in SAS JMP Pro, I have included the concepts and reworked it using Python and its packages. . Literature . Some useful literature for reading . Public Transport OD matrices | Identifying Functional Regions in Australia Using Hierarchical Aggregation Techniques | . Getting the data . Data can be found via the Land Transport Authority Data Mall via an API call. I have also included the codes for API call but the accountkey have to be requested. . import pandas as pd import numpy as np import requests import seaborn as sns import matplotlib.pyplot as plt from pandas_profiling import ProfileReport . # import requests # def get_data(): # url = &#39;http://datamall2.mytransport.sg/ltaodataservice/PV/ODTrain&#39; # headers = {&#39;AccountKey&#39;:&#39;your key here&#39;,&#39;accept&#39;:&#39;application/json&#39;} # payload = {&#39;Date&#39;:YYYYMM} # return requests.get(url,params=payload,headers=headers).json() # link = get_data()[&#39;value&#39;][0][&#39;Link&#39;] # # extract csv from link for the month&#39;s data . df = pd.read_csv(&#39;origin_destination_train_202205.csv&#39;) df = df.drop([&#39;YEAR_MONTH&#39;,&#39;PT_TYPE&#39;],axis=1) . Displaying MRT stations and lines . For the purpose of visual displays, we will make use of the python package Folium to create a nice display of all the train stations. The coordinates of MRT stations as of now can be found pretty much anywhere, but i have used this repo and added new stations manually. . . Note: Folium does not work with fastpages for now. will look into using plotly or altair as a workaround. If need to, you can render using https://nbviewer.org/ with link for the interactive elements. Have included the png image for now for all folium plots . import folium from folium.plugins import MarkerCluster . mrt = pd.read_csv(&#39;mrt.csv&#39;) def add_markers(stations,map,colour=&#39;default&#39;): for stn in stations: stn_coor = list(mrt.loc[mrt[&#39;STN_NO&#39;]==stn,[&#39;Latitude&#39;,&#39;Longitude&#39;]].values[0]) if colour==&#39;default&#39;: stn_color = mrt.loc[mrt[&#39;STN_NO&#39;]==stn,[&#39;COLOR&#39;]].values[0][0].lower() else: stn_color = colour stn_full_name = mrt.loc[mrt[&#39;STN_NO&#39;]==stn,[&#39;STN_NAME&#39;]].values[0][0] if (ind1 := stn_full_name.find(&#39;MRT&#39;)) &gt; -1: stn_name = stn_full_name[:ind1-1].title() elif (ind2 := stn_full_name.find(&#39;LRT&#39;)) &gt; -1: stn_name = stn_full_name[:ind2-1].title() else: stn_name = stn_full_name if stn_color == &#39;gray&#39;: stn_color = &#39;lightgray&#39; #print(stn_coor,stn_color,stn) folium.Marker(location=stn_coor, popup = stn+&#39; &#39;+stn_name,icon=folium.Icon(color=stn_color)).add_to(map) . #Define coordinates of where we want to center our map sg = [1.3561519863229077, 103.80586415619761] #Create the map my_map = folium.Map(location = sg, zoom_start = 11) #add markers add_markers(mrt.STN_NO.to_list(),my_map,colour=&#39;default&#39;) # #Display the map my_map . Make this Notebook Trusted to load map: File -&gt; Trust Notebook . Let&#39;s try to visualize the MRT systems maps by colour. Since folium doesn&#39;t have the color brown, I have used pink instead for the TEL line . . Exploratory Data Analysis . A useful package for basic EDA is through Pandas Profiler. You may visit their project website for more details/ . file = ProfileReport(df) file.to_notebook_iframe() . . Weekday number of total trips are higher than that of weekends. Mean trips for all stations are higher at 93 vs 50 for weekends. Peak OD is also highest for weekdays vs weekends . df.groupby(&#39;DAY_TYPE&#39;).describe()[&#39;TOTAL_TRIPS&#39;] . count mean std min 25% 50% 75% max . DAY_TYPE . WEEKDAY 363723.0 | 93.888036 | 303.988433 | 1.0 | 4.0 | 15.0 | 59.0 | 12798.0 | . WEEKENDS/HOLIDAY 336106.0 | 50.504823 | 159.810852 | 1.0 | 3.0 | 9.0 | 32.0 | 5442.0 | . Let&#39;s try to find out the total trips by time for all stations for all days . df.groupby(&#39;TIME_PER_HOUR&#39;).describe() . TOTAL_TRIPS . count mean std min 25% 50% 75% max . TIME_PER_HOUR . 0 3884.0 | 9.386972 | 20.068909 | 1.0 | 1.0 | 3.0 | 8.0 | 230.0 | . 5 19572.0 | 27.882485 | 78.796186 | 1.0 | 2.0 | 6.0 | 21.0 | 2067.0 | . 6 32611.0 | 73.470087 | 244.369474 | 1.0 | 3.0 | 13.0 | 49.0 | 7058.0 | . 7 37441.0 | 108.842739 | 372.845040 | 1.0 | 4.0 | 17.0 | 66.0 | 10811.0 | . 8 38314.0 | 93.533669 | 310.211684 | 1.0 | 4.0 | 15.0 | 57.0 | 8510.0 | . 9 37626.0 | 64.812018 | 188.997212 | 1.0 | 3.0 | 11.0 | 43.0 | 4904.0 | . 10 37812.0 | 58.559346 | 174.290314 | 1.0 | 3.0 | 10.0 | 37.0 | 4580.0 | . 11 38699.0 | 60.849273 | 187.334462 | 1.0 | 3.0 | 10.0 | 38.0 | 4662.0 | . 12 39760.0 | 66.604930 | 209.799559 | 1.0 | 3.0 | 11.0 | 42.0 | 5523.0 | . 13 39870.0 | 69.337848 | 219.528833 | 1.0 | 3.0 | 12.0 | 43.0 | 5585.0 | . 14 39623.0 | 65.880776 | 204.457741 | 1.0 | 3.0 | 11.0 | 42.0 | 5380.0 | . 15 39987.0 | 68.949584 | 211.852530 | 1.0 | 4.0 | 12.0 | 45.0 | 6510.0 | . 16 40995.0 | 76.218392 | 233.303667 | 1.0 | 4.0 | 14.0 | 50.0 | 8898.0 | . 17 42318.0 | 106.590434 | 333.401763 | 1.0 | 5.0 | 18.0 | 69.0 | 10503.0 | . 18 41708.0 | 120.898125 | 392.323923 | 1.0 | 5.0 | 18.0 | 73.0 | 12798.0 | . 19 39260.0 | 84.383597 | 269.718346 | 1.0 | 4.0 | 14.0 | 52.0 | 8755.0 | . 20 37155.0 | 66.658135 | 220.745056 | 1.0 | 3.0 | 11.0 | 40.0 | 7971.0 | . 21 35917.0 | 61.105020 | 204.300373 | 1.0 | 3.0 | 10.0 | 37.0 | 7423.0 | . 22 33151.0 | 43.948237 | 131.877848 | 1.0 | 2.0 | 8.0 | 29.0 | 3776.0 | . 23 24126.0 | 24.061428 | 67.571074 | 1.0 | 2.0 | 5.0 | 17.0 | 2865.0 | . Morning Peak and PM Peak have the highest mean numbers, not surprisingly. We try to separate it by weekdays and weekends to see if there is any difference . df_total_trips = df.groupby([&#39;DAY_TYPE&#39;,&#39;TIME_PER_HOUR&#39;]).describe()[&#39;TOTAL_TRIPS&#39;][[&#39;count&#39;,&#39;50%&#39;,&#39;mean&#39;]].rename(columns={&#39;50%&#39;:&#39;median&#39;}).reset_index() df_total_trips=df_total_trips.melt(id_vars=[&#39;DAY_TYPE&#39;,&#39;TIME_PER_HOUR&#39;],var_name=&#39;count_type&#39;) g = sns.catplot(data=df_total_trips,x=&#39;TIME_PER_HOUR&#39;,y=&#39;value&#39;,col=&#39;DAY_TYPE&#39;, row=&#39;count_type&#39;,kind=&#39;bar&#39;,row_order=[&#39;count&#39;,&#39;mean&#39;,&#39;median&#39;],sharey=False,color=&#39;lightblue&#39;) # for ax in g.axes: # ax.tick_params(labelbottom=True) plt.show() . . We plot the graphs of the total trips by hours for both weekday and weekends. Not surprisingly, the mean and median trips are the highest during the AM and PM peak during weekdays. For weekends, the number of trips are generally constant throughout the day, with smaller number at the beginning and end of day. . How about the total trips by stations? We can have 2 groups, by origin and by destination. First, by origin. . df.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].mean().reset_index().sort_values(&#39;TOTAL_TRIPS&#39;,ascending=False) . ORIGIN_PT_CODE TOTAL_TRIPS . 76 EW24/NS1 | 259.187846 | . 119 NS21/DT11 | 208.347993 | . 72 EW2/DT32 | 205.494127 | . 120 NS22 | 203.382958 | . 110 NS13 | 199.930361 | . ... ... | ... | . 160 TE5 | 12.613582 | . 142 PW6 | 12.354680 | . 152 SW4 | 11.462328 | . 138 PW1 | 9.931076 | . 139 PW3 | 6.302691 | . 164 rows × 2 columns . plt.figure(figsize=(25,15)) g=sns.barplot(data=df.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].mean().reset_index(),x=&#39;ORIGIN_PT_CODE&#39;,y=&#39;TOTAL_TRIPS&#39;,color=&#39;lightblue&#39;) plt.xticks(rotation=90) plt.title(&#39;Mean trips by origin&#39;) plt.show() . . For analysis of the number of trips by origin, we can clearly see there are some areas that have higher volumes, for example the top 3 volume are at EW24/NS1 (Jurong East) at 259 trips, NS21/DT11 (Newton) at 208 trips and EW2/DT32 (Tampines) at 205 trips. . df.groupby(&#39;DESTINATION_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].mean().reset_index().sort_values(&#39;TOTAL_TRIPS&#39;,ascending=False) . DESTINATION_PT_CODE TOTAL_TRIPS . 76 EW24/NS1 | 261.409655 | . 120 NS22 | 220.397902 | . 130 NS9/TE2 | 220.258299 | . 119 NS21/DT11 | 214.947722 | . 110 NS13 | 210.738939 | . ... ... | ... | . 124 NS28 | 12.187382 | . 152 SW4 | 11.871693 | . 142 PW6 | 11.335381 | . 138 PW1 | 9.417886 | . 139 PW3 | 7.210399 | . 164 rows × 2 columns . plt.figure(figsize=(25,15)) g=sns.barplot(data=df.groupby(&#39;DESTINATION_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].mean().reset_index(),x=&#39;DESTINATION_PT_CODE&#39;,y=&#39;TOTAL_TRIPS&#39;,color=&#39;lightblue&#39;) plt.xticks(rotation=90) plt.title(&#39;Mean trips by destination&#39;) plt.show() . . Similarly for number of trips by destination, the top 3 volume are at EW24/NS1 (Jurong East) at 261 trips, NS22 (Orchard) at 220 trips and NS9/TE2 (Woodlands) at 220 trips. . Weekday AM Peak (7-9am) . Preparing data into OD matrix . Segmentise into weekday morning peak, weekday evening peak, weekends peak | sum of total trips by origin and destination | standardisation of data (min-max scaler) with sum of sum of origin and destination | . weekday_am_peak = df[(df[&#39;DAY_TYPE&#39;]==&#39;WEEKDAY&#39;) &amp; (df[&#39;TIME_PER_HOUR&#39;]&lt;=9) &amp; (df[&#39;TIME_PER_HOUR&#39;]&gt;=7)].drop(columns=[&#39;DAY_TYPE&#39;]) weekday_am_peak=weekday_am_peak.groupby([&#39;ORIGIN_PT_CODE&#39;,&#39;DESTINATION_PT_CODE&#39;]).sum().reset_index().drop(columns=[&#39;TIME_PER_HOUR&#39;]) . # To transform and find out the total_trips per pair as a pct of trips by the sum of trips from each origin station weekday_am_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekday_am_peak[&#39;sum_total_trips_origin&#39;]=weekday_am_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekday_am_peak[&#39;pct_trips_sum_origin&#39;]=weekday_am_peak[&#39;TOTAL_TRIPS&#39;]/weekday_am_peak[&#39;sum_total_trips_origin&#39;] #Creating OD matrix with origins as rows and destinations as columns, values as the normalised trips by origin od_weekdayam_mat = weekday_am_peak.pivot(index=&#39;ORIGIN_PT_CODE&#39;,columns=&#39;DESTINATION_PT_CODE&#39;,values=&#39;pct_trips_sum_origin&#39;).fillna(0) od_weekdayam_mat . DESTINATION_PT_CODE BP10 BP11 BP12 BP13 BP2 BP3 BP4 BP5 BP6/DT1 BP7 ... SW6 SW7 SW8 TE1 TE3 TE4 TE5 TE6 TE7 TE8 . ORIGIN_PT_CODE . BP10 0.000000 | 0.019253 | 0.046300 | 0.019304 | 0.005552 | 0.008863 | 0.033464 | 0.042429 | 0.356390 | 0.022819 | ... | 0.0 | 0.0 | 0.000000 | 0.007793 | 0.000306 | 0.001019 | 0.000611 | 0.000509 | 0.001121 | 0.000102 | . BP11 0.011440 | 0.000000 | 0.010368 | 0.013025 | 0.006255 | 0.010711 | 0.037875 | 0.031234 | 0.404542 | 0.013239 | ... | 0.0 | 0.0 | 0.000000 | 0.014781 | 0.001371 | 0.000171 | 0.000300 | 0.000000 | 0.000086 | 0.000086 | . BP12 0.021518 | 0.008227 | 0.000000 | 0.008803 | 0.007882 | 0.011219 | 0.041252 | 0.024912 | 0.427881 | 0.008170 | ... | 0.0 | 0.0 | 0.000000 | 0.014844 | 0.000288 | 0.000000 | 0.000000 | 0.000230 | 0.000173 | 0.000058 | . BP13 0.039606 | 0.017033 | 0.045044 | 0.000000 | 0.018059 | 0.012415 | 0.047917 | 0.011492 | 0.222450 | 0.003489 | ... | 0.0 | 0.0 | 0.000000 | 0.016827 | 0.000205 | 0.000103 | 0.000000 | 0.000000 | 0.000103 | 0.000000 | . BP2 0.010395 | 0.004921 | 0.014212 | 0.005243 | 0.000000 | 0.007359 | 0.024791 | 0.018398 | 0.345690 | 0.004507 | ... | 0.0 | 0.0 | 0.000000 | 0.015730 | 0.000184 | 0.000966 | 0.000000 | 0.000092 | 0.001380 | 0.001058 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . TE4 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000907 | 0.006166 | 0.000000 | 0.000363 | 0.000000 | ... | 0.0 | 0.0 | 0.000544 | 0.106456 | 0.054044 | 0.000000 | 0.019405 | 0.052956 | 0.087777 | 0.060936 | . TE5 0.000000 | 0.000000 | 0.000098 | 0.000000 | 0.000000 | 0.000295 | 0.007864 | 0.001180 | 0.000491 | 0.000295 | ... | 0.0 | 0.0 | 0.000000 | 0.153642 | 0.026639 | 0.023690 | 0.000000 | 0.029981 | 0.053868 | 0.036371 | . TE6 0.000036 | 0.000000 | 0.000107 | 0.000071 | 0.000000 | 0.000285 | 0.006656 | 0.000356 | 0.002598 | 0.000036 | ... | 0.0 | 0.0 | 0.000036 | 0.122762 | 0.018473 | 0.013561 | 0.008578 | 0.000000 | 0.078946 | 0.040648 | . TE7 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000267 | 0.003830 | 0.000000 | 0.001514 | 0.000000 | ... | 0.0 | 0.0 | 0.000000 | 0.074826 | 0.018350 | 0.030376 | 0.019776 | 0.087386 | 0.000000 | 0.042758 | . TE8 0.000000 | 0.000000 | 0.000261 | 0.000000 | 0.000000 | 0.000000 | 0.001172 | 0.000000 | 0.001693 | 0.000000 | ... | 0.0 | 0.0 | 0.000130 | 0.071382 | 0.020060 | 0.017846 | 0.015110 | 0.065260 | 0.041032 | 0.000000 | . 164 rows × 164 columns . . import scipy.cluster.hierarchy as shc from sklearn.cluster import AgglomerativeClustering . # # hierarchical clustering and display dendogram def plot_dendrogram(data: pd.DataFrame, **kwargs): clusters = shc.linkage(data,**kwargs) plt.figure(figsize=(25,16)) plt.title(&quot;Hierarchical Clustering Dendrogram&quot;) shc.dendrogram(Z=clusters,leaf_font_size=8,labels=data.index) plt.axhline(y = 0.53, color = &#39;r&#39;, linestyle = &#39;-&#39;) plt.show() plot_dendrogram(od_weekdayam_mat,method=&#39;ward&#39;,metric=&#39;euclidean&#39;) . . Finding the optimal cluster size in agglomerative hierarchical clustering is to find out the largest horizontal space that doesn&#39;t have any vertical lines(space with the longest vertical lines), with the number of clusters being the vertical lines that cut the horizontal line. This meant that the separation between clusters are the largest. We can draw the horizontal line by eyeballing the y values. Here, the number of times that the horizontal line cuts through is 8. . As the default euclidean distance metric with ward linkages works well, we will stick to the defaults. We also mentioned the default values here for simplicity of changing the linkage and affinity in order to tweak them later if needed. . def agg_cluster_fit(data: pd.DataFrame,cluster_size: int, **kwargs): model = AgglomerativeClustering(n_clusters=cluster_size,**kwargs) #affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39; model.fit(data) data[&#39;label&#39;] = model.labels_ d = [&#39;,&#39;.join(data[data[&#39;label&#39;]==x].index) for x in range(cluster_size)] df = pd.DataFrame(index=range(1,cluster_size+1),columns=[&#39;Stations&#39;],data=d) return df weekday_am_peak_clusters = agg_cluster_fit(data=od_weekdayam_mat,cluster_size=8,affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39;) weekday_am_peak_clusters.style.set_properties(**{&#39;text-align&#39;: &#39;left&#39;}) . &nbsp; Stations . 1 NE1/CC29,NE10,NE11,NE12/CC13,NE13,NE14,NE15,NE16/STC,NE17/PTC,NE5,NE7/DT12,NE8,NE9,NS24/NE6/CC1,PE1,PE2,PE3,PE4,PE5,PE6,PE7,PW1,PW3,PW4,PW5,PW6,PW7,SE1,SE2,SE3,SE4,SE5,SW1,SW2,SW3,SW4,SW5,SW6,SW7,SW8 | . 2 CC17/TE9,CC19/DT9,CC20,CC21,CC23,CC24,CC25,CC26,CC27,CC28,TE1,TE3,TE4,TE5,TE6,TE7,TE8 | . 3 CC22/EW21,EW17,EW18,EW19,EW20,EW22,EW23,EW24/NS1,EW25,EW26,EW27,EW28,EW29,EW30,EW31,EW32,EW33,NS2,NS3,NS4/BP1,NS5 | . 4 BP10,BP11,BP12,BP13,BP2,BP3,BP4,BP5,BP7,BP8,BP9 | . 5 CC4/DT15,CC6,CG1/DT35,DT20,DT21,DT22,DT23,DT24,DT25,DT27,DT28,DT29,DT30,DT31,DT33,DT34 | . 6 CC10/DT26,CC11,CC12,CC14,CC16,CC2,CC3,CC5,CC7,CC8,CG2,DT17,DT18,EW1,EW10,EW11,EW12/DT14,EW13/NS25,EW14/NS26,EW15,EW16/NE3,EW2/DT32,EW3,EW4,EW5,EW6,EW7,EW8/CC9,EW9,NE4/DT19,NS21/DT11,NS27/CE2,NS28 | . 7 NS10,NS11,NS12,NS13,NS14,NS15,NS16,NS17/CC15,NS18,NS19,NS20,NS22,NS23,NS7,NS8,NS9/TE2 | . 8 BP6/DT1,CE1/DT16,DT10,DT13,DT2,DT3,DT5,DT6,DT7,DT8 | . Lets try to visualize the clusters on a map. Each cluster is set to a different colour so that we can identify the stations in a cluster easily. . # set a colour for each cluster weekday_am_peak_clusters[&#39;cluster_colour&#39;]=[&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;darkred&#39;, &#39;lightred&#39;, &#39;beige&#39;] #Define coordinates of where we want to center our map sg = [1.3561519863229077, 103.80586415619761] #Create the map my_map = folium.Map(location = sg, zoom_start = 12) #Load list of stations mrt = pd.read_csv(&#39;mrt.csv&#39;) #add markers for i in range(len(weekday_am_peak_clusters)): cluster_list = weekday_am_peak_clusters.iloc[i].to_list()[0].split(&#39;,&#39;) colour = weekday_am_peak_clusters.iloc[i].to_list()[1] add_markers(cluster_list,my_map,colour=colour) # #Display the map my_map . Make this Notebook Trusted to load map: File -&gt; Trust Notebook . . We can see that the clustering algorithm strongly favours MRT/LRT stations that lies on the same MRT/LRT line, and also within the same region in Singapore. . Weekday PM Peak (5-7pm) . weekday_pm_peak = df[(df[&#39;DAY_TYPE&#39;]==&#39;WEEKDAY&#39;) &amp; (df[&#39;TIME_PER_HOUR&#39;]&lt;=19) &amp; (df[&#39;TIME_PER_HOUR&#39;]&gt;=17)].drop(columns=[&#39;DAY_TYPE&#39;]) weekday_pm_peak=weekday_pm_peak.groupby([&#39;ORIGIN_PT_CODE&#39;,&#39;DESTINATION_PT_CODE&#39;]).sum().reset_index().drop(columns=[&#39;TIME_PER_HOUR&#39;]) . # To transform and find out the total_trips per pair as a pct of trips by the sum of trips from each origin station weekday_pm_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekday_pm_peak[&#39;sum_total_trips_origin&#39;]=weekday_pm_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekday_pm_peak[&#39;pct_trips_sum_origin&#39;]=weekday_pm_peak[&#39;TOTAL_TRIPS&#39;]/weekday_pm_peak[&#39;sum_total_trips_origin&#39;] #Creating OD matrix with origins as rows and destinations as columns, values as the normalised trips by origin od_weekdaypm_mat = weekday_pm_peak.pivot(index=&#39;ORIGIN_PT_CODE&#39;,columns=&#39;DESTINATION_PT_CODE&#39;,values=&#39;pct_trips_sum_origin&#39;).fillna(0) od_weekdaypm_mat . DESTINATION_PT_CODE BP10 BP11 BP12 BP13 BP2 BP3 BP4 BP5 BP6/DT1 BP7 ... SW6 SW7 SW8 TE1 TE3 TE4 TE5 TE6 TE7 TE8 . ORIGIN_PT_CODE . BP10 0.000000 | 0.047719 | 0.062879 | 0.051147 | 0.026364 | 0.034142 | 0.017137 | 0.031901 | 0.268521 | 0.052333 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000264 | 0.000264 | 0.000000 | 0.000000 | 0.000264 | 0.000000 | 0.000000 | . BP11 0.037673 | 0.000000 | 0.027200 | 0.028509 | 0.035636 | 0.028218 | 0.039127 | 0.048727 | 0.318545 | 0.021527 | ... | 0.000000 | 0.000000 | 0.000000 | 0.001018 | 0.000145 | 0.000000 | 0.000000 | 0.000145 | 0.000000 | 0.000000 | . BP12 0.031290 | 0.019317 | 0.000000 | 0.028576 | 0.026341 | 0.036398 | 0.023308 | 0.033046 | 0.381386 | 0.018199 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000160 | 0.000479 | 0.000160 | 0.000160 | 0.000958 | 0.000319 | 0.000000 | . BP13 0.083534 | 0.057993 | 0.029748 | 0.000000 | 0.043570 | 0.037560 | 0.019531 | 0.022236 | 0.233173 | 0.006611 | ... | 0.000000 | 0.000000 | 0.000000 | 0.002103 | 0.000000 | 0.000300 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . BP2 0.026615 | 0.027393 | 0.024903 | 0.018988 | 0.000000 | 0.040311 | 0.018833 | 0.032840 | 0.261323 | 0.012296 | ... | 0.000000 | 0.000000 | 0.000000 | 0.001245 | 0.000934 | 0.000156 | 0.000311 | 0.000156 | 0.000000 | 0.000467 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . TE4 0.000644 | 0.000322 | 0.000161 | 0.000000 | 0.002739 | 0.003867 | 0.001450 | 0.001208 | 0.002175 | 0.000000 | ... | 0.000000 | 0.000081 | 0.000000 | 0.011278 | 0.074996 | 0.000000 | 0.023925 | 0.045433 | 0.025858 | 0.037699 | . TE5 0.001775 | 0.000197 | 0.000197 | 0.000000 | 0.000592 | 0.001183 | 0.000197 | 0.001775 | 0.002564 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.007692 | 0.114201 | 0.042801 | 0.000000 | 0.027219 | 0.032742 | 0.114004 | . TE6 0.000093 | 0.000186 | 0.000093 | 0.000000 | 0.000371 | 0.000928 | 0.000835 | 0.001485 | 0.004175 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000093 | 0.012247 | 0.086380 | 0.030803 | 0.035071 | 0.000000 | 0.061514 | 0.131286 | . TE7 0.000659 | 0.000366 | 0.000220 | 0.000000 | 0.000512 | 0.001610 | 0.000586 | 0.000000 | 0.007173 | 0.000000 | ... | 0.000586 | 0.000805 | 0.000000 | 0.003952 | 0.031618 | 0.034473 | 0.037034 | 0.107297 | 0.000000 | 0.088121 | . TE8 0.000000 | 0.000054 | 0.000107 | 0.000000 | 0.001180 | 0.001180 | 0.000161 | 0.000000 | 0.013141 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000161 | 0.005793 | 0.048917 | 0.028588 | 0.046181 | 0.126904 | 0.095259 | 0.000000 | . 164 rows × 164 columns . . # hierarchical clustering and display dendogram def plot_dendrogram(data: pd.DataFrame, **kwargs): clusters = shc.linkage(data,**kwargs) plt.figure(figsize=(25,16)) plt.title(&quot;Hierarchical Clustering Dendrogram&quot;) shc.dendrogram(Z=clusters,leaf_font_size=8,labels=data.index) plt.axhline(y = 0.7, color = &#39;r&#39;, linestyle = &#39;-&#39;) plt.show() plot_dendrogram(od_weekdaypm_mat,method=&#39;ward&#39;,metric=&#39;euclidean&#39;) . . Optimal cluster size is about 6 . def agg_cluster_fit(data: pd.DataFrame,cluster_size: int, **kwargs): model = AgglomerativeClustering(n_clusters=cluster_size,**kwargs) #affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39; model.fit(data) data[&#39;label&#39;] = model.labels_ d = [&#39;,&#39;.join(data[data[&#39;label&#39;]==x].index) for x in range(cluster_size)] df = pd.DataFrame(index=range(1,cluster_size+1),columns=[&#39;Stations&#39;],data=d) return df weekday_pm_peak_clusters = agg_cluster_fit(data=od_weekdaypm_mat,cluster_size=6,affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39;) weekday_pm_peak_clusters.style.set_properties(**{&#39;text-align&#39;: &#39;left&#39;}) . &nbsp; Stations . 1 CC10/DT26,CC11,CC12,CC14,CC16,CC17/TE9,CC19/DT9,CC2,CC20,CC21,CC23,CC24,CC25,CC26,CC27,CC28,CC3,CC4/DT15,CC5,CC6,CC7,CC8,CE1/DT16,DT10,DT13,DT17,DT18,DT2,DT3,DT5,DT6,DT7,DT8,EW12/DT14,EW16/NE3,NE1/CC29,NE10,NE11,NE12/CC13,NE13,NE14,NE15,NE16/STC,NE17/PTC,NE4/DT19,NE5,NE7/DT12,NE8,NE9,NS24/NE6/CC1,PW3 | . 2 CC22/EW21,EW11,EW13/NS25,EW14/NS26,EW15,EW17,EW18,EW19,EW20,EW22,EW23,EW24/NS1,EW25,EW26,EW27,EW28,EW29,EW30,EW31,EW32,EW33,NS10,NS11,NS12,NS13,NS14,NS15,NS16,NS17/CC15,NS18,NS19,NS2,NS20,NS21/DT11,NS22,NS23,NS27/CE2,NS28,NS3,NS4/BP1,NS5,NS7,NS8,NS9/TE2,TE1,TE3,TE4,TE5,TE6,TE7,TE8 | . 3 CG1/DT35,CG2,DT20,DT21,DT22,DT23,DT24,DT25,DT27,DT28,DT29,DT30,DT31,DT33,DT34,EW1,EW10,EW2/DT32,EW3,EW4,EW5,EW6,EW7,EW8/CC9,EW9 | . 4 PE1,PE2,PE3,PE4,PE5,PE6,PE7,PW1,PW4,PW5,PW6,PW7 | . 5 SE1,SE2,SE3,SE4,SE5,SW1,SW2,SW3,SW4,SW5,SW6,SW7,SW8 | . 6 BP10,BP11,BP12,BP13,BP2,BP3,BP4,BP5,BP6/DT1,BP7,BP8,BP9 | . Lets try to visualize the clusters on a map. Each cluster is set to a different colour so that we can identify the stations in a cluster easily. . # set a colour for each cluster weekday_pm_peak_clusters[&#39;cluster_colour&#39;]=[&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;darkred&#39;] #Define coordinates of where we want to center our map sg = [1.3561519863229077, 103.80586415619761] #Create the map my_map = folium.Map(location = sg, zoom_start = 12) #Load list of stations mrt = pd.read_csv(&#39;mrt.csv&#39;) #add markers for i in range(len(weekday_pm_peak_clusters)): cluster_list = weekday_pm_peak_clusters.iloc[i].to_list()[0].split(&#39;,&#39;) colour = weekday_pm_peak_clusters.iloc[i].to_list()[1] add_markers(cluster_list,my_map,colour=colour) # #Display the map my_map . Make this Notebook Trusted to load map: File -&gt; Trust Notebook . . Weekends Noon (11-1pm) . weekend_peak = df[(df[&#39;DAY_TYPE&#39;]==&#39;WEEKDAY&#39;) &amp; (df[&#39;TIME_PER_HOUR&#39;]&lt;=13) &amp; (df[&#39;TIME_PER_HOUR&#39;]&gt;=11)].drop(columns=[&#39;DAY_TYPE&#39;]) weekend_peak=weekend_peak.groupby([&#39;ORIGIN_PT_CODE&#39;,&#39;DESTINATION_PT_CODE&#39;]).sum().reset_index().drop(columns=[&#39;TIME_PER_HOUR&#39;]) . # To transform and find out the total_trips per pair as a pct of trips by the sum of trips from each origin station weekend_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekend_peak[&#39;sum_total_trips_origin&#39;]=weekend_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekend_peak[&#39;pct_trips_sum_origin&#39;]=weekend_peak[&#39;TOTAL_TRIPS&#39;]/weekend_peak[&#39;sum_total_trips_origin&#39;] #Creating OD matrix with origins as rows and destinations as columns, values as the normalised trips by origin od_weekend_mat = weekend_peak.pivot(index=&#39;ORIGIN_PT_CODE&#39;,columns=&#39;DESTINATION_PT_CODE&#39;,values=&#39;pct_trips_sum_origin&#39;).fillna(0) od_weekend_mat . DESTINATION_PT_CODE BP10 BP11 BP12 BP13 BP2 BP3 BP4 BP5 BP6/DT1 BP7 ... SW6 SW7 SW8 TE1 TE3 TE4 TE5 TE6 TE7 TE8 . ORIGIN_PT_CODE . BP10 0.000000 | 0.053183 | 0.063512 | 0.042993 | 0.017588 | 0.040620 | 0.031407 | 0.045505 | 0.380793 | 0.020519 | ... | 0.000000 | 0.000000 | 0.00000 | 0.000140 | 0.000419 | 0.000000 | 0.000000 | 0.000000 | 0.000279 | 0.000000 | . BP11 0.001004 | 0.000000 | 0.018936 | 0.029981 | 0.022809 | 0.040597 | 0.037441 | 0.044326 | 0.386028 | 0.003730 | ... | 0.000000 | 0.000000 | 0.00000 | 0.000430 | 0.000287 | 0.000000 | 0.000000 | 0.000143 | 0.000000 | 0.000143 | . BP12 0.010195 | 0.000703 | 0.000000 | 0.044176 | 0.028123 | 0.033161 | 0.029880 | 0.035036 | 0.488985 | 0.017928 | ... | 0.000000 | 0.000000 | 0.00000 | 0.001406 | 0.000117 | 0.000000 | 0.000000 | 0.000703 | 0.000000 | 0.000117 | . BP13 0.007932 | 0.004271 | 0.000305 | 0.000000 | 0.028981 | 0.067114 | 0.061318 | 0.021660 | 0.324283 | 0.009457 | ... | 0.000000 | 0.000000 | 0.00000 | 0.001220 | 0.000305 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . BP2 0.021167 | 0.016517 | 0.004490 | 0.004811 | 0.000000 | 0.044419 | 0.023573 | 0.024375 | 0.314144 | 0.015555 | ... | 0.000000 | 0.000000 | 0.00016 | 0.001443 | 0.000481 | 0.000000 | 0.000000 | 0.000160 | 0.000000 | 0.000481 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . TE4 0.000238 | 0.000475 | 0.000000 | 0.000000 | 0.000475 | 0.000713 | 0.001900 | 0.000000 | 0.001663 | 0.000000 | ... | 0.000000 | 0.000238 | 0.00000 | 0.020190 | 0.079335 | 0.000000 | 0.046793 | 0.062470 | 0.057957 | 0.087173 | . TE5 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000759 | 0.004556 | 0.000000 | 0.000759 | 0.000000 | ... | 0.000000 | 0.000000 | 0.00000 | 0.020121 | 0.040243 | 0.061503 | 0.000000 | 0.050114 | 0.064920 | 0.211086 | . TE6 0.000000 | 0.000000 | 0.000173 | 0.000086 | 0.000000 | 0.000778 | 0.003198 | 0.000000 | 0.002938 | 0.000086 | ... | 0.000086 | 0.000000 | 0.00000 | 0.015124 | 0.033273 | 0.029384 | 0.043039 | 0.000000 | 0.117103 | 0.133005 | . TE7 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000750 | 0.001124 | 0.000000 | 0.004497 | 0.000250 | ... | 0.000250 | 0.000125 | 0.00000 | 0.003623 | 0.026608 | 0.048595 | 0.060837 | 0.186633 | 0.000000 | 0.160400 | . TE8 0.000000 | 0.000000 | 0.000128 | 0.000000 | 0.000000 | 0.000255 | 0.000000 | 0.000000 | 0.006384 | 0.000000 | ... | 0.000128 | 0.000128 | 0.00000 | 0.008427 | 0.028218 | 0.032942 | 0.060904 | 0.114785 | 0.145940 | 0.000000 | . 164 rows × 164 columns . . # hierarchical clustering and display dendogram def plot_dendrogram(data: pd.DataFrame, **kwargs): clusters = shc.linkage(data,**kwargs) plt.figure(figsize=(25,16)) plt.title(&quot;Hierarchical Clustering Dendrogram&quot;) shc.dendrogram(Z=clusters,leaf_font_size=8,labels=data.index) plt.axhline(y = 0.82, color = &#39;r&#39;, linestyle = &#39;-&#39;) plt.show() plot_dendrogram(od_weekend_mat,method=&#39;ward&#39;,metric=&#39;euclidean&#39;) . . Optimal cluster size is about 7 . def agg_cluster_fit(data: pd.DataFrame,cluster_size: int, **kwargs): model = AgglomerativeClustering(n_clusters=cluster_size,**kwargs) #affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39; model.fit(data) data[&#39;label&#39;] = model.labels_ d = [&#39;,&#39;.join(data[data[&#39;label&#39;]==x].index) for x in range(cluster_size)] df = pd.DataFrame(index=range(1,cluster_size+1),columns=[&#39;Stations&#39;],data=d) return df weekend_peak_clusters = agg_cluster_fit(data=od_weekend_mat,cluster_size=7,affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39;) weekend_peak_clusters.style.set_properties(**{&#39;text-align&#39;: &#39;left&#39;}) . &nbsp; Stations . 1 BP6/DT1,CC10/DT26,CC11,CC12,CC14,CC16,CC17/TE9,CC19/DT9,CC2,CC20,CC21,CC22/EW21,CC23,CC24,CC25,CC26,CC27,CC28,CC3,CC4/DT15,CC5,CC6,CC7,CC8,CG2,DT10,DT13,DT3,DT5,DT6,DT7,DT8,EW10,EW11,EW12/DT14,EW13/NS25,EW14/NS26,EW15,EW16/NE3,EW2/DT32,EW4,EW5,EW6,EW7,EW8/CC9,EW9,NE1/CC29,NE10,NE11,NE12/CC13,NE13,NE14,NE15,NE16/STC,NE17/PTC,NE4/DT19,NE5,NE7/DT12,NE8,NE9,NS10,NS11,NS12,NS13,NS14,NS15,NS16,NS17/CC15,NS18,NS19,NS20,NS21/DT11,NS22,NS23,NS24/NE6/CC1,NS27/CE2,NS28,NS7,NS8,NS9/TE2 | . 2 BP10,BP11,BP12,BP13,BP2,BP3,BP4,BP5,BP7,BP8,BP9,DT2 | . 3 CE1/DT16,CG1/DT35,DT17,DT18,DT20,DT21,DT22,DT23,DT24,DT25,DT27,DT28,DT29,DT30,DT31,DT33,DT34,EW1,EW3 | . 4 SE1,SE2,SE3,SE4,SE5,SW1,SW2,SW3,SW4,SW5,SW6,SW7,SW8 | . 5 TE1,TE3,TE4,TE5,TE6,TE7,TE8 | . 6 PE1,PE2,PE3,PE4,PE5,PE6,PE7,PW1,PW3,PW4,PW5,PW6,PW7 | . 7 EW17,EW18,EW19,EW20,EW22,EW23,EW24/NS1,EW25,EW26,EW27,EW28,EW29,EW30,EW31,EW32,EW33,NS2,NS3,NS4/BP1,NS5 | . Lets try to visualize the clusters on a map. Each cluster is set to a different colour so that we can identify the stations in a cluster easily. . # set a colour for each cluster weekend_peak_clusters[&#39;cluster_colour&#39;]=[&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;darkred&#39;, &#39;lightred&#39;] #Define coordinates of where we want to center our map sg = [1.3561519863229077, 103.80586415619761] #Create the map my_map = folium.Map(location = sg, zoom_start = 12) #Load list of stations mrt = pd.read_csv(&#39;mrt.csv&#39;) #add markers for i in range(len(weekend_peak_clusters)): cluster_list = weekend_peak_clusters.iloc[i].to_list()[0].split(&#39;,&#39;) colour = weekend_peak_clusters.iloc[i].to_list()[1] add_markers(cluster_list,my_map,colour=colour) # #Display the map my_map . Make this Notebook Trusted to load map: File -&gt; Trust Notebook . . Closing Thoughts . Since this was one of the first assignments that I have done in my data science journey, there are several items that I may could have done in addition. For example, we could have done clustering analysis for both origin and destinations to uncover the clusters in both direction. This could have revealed interesting insights on travel patterns. Also, I could have only used data from major hubs to identify where people are working, and if people choose their work location nearer to their homes and the extent of it. We could also have used PCA/SVD analysis to further identify possible number of clusters. Also, I could have applied time series analysis to identify trends within a month or across months, and especially during pre and periods of COVID-19 relaxation periods. .",
            "url": "https://calamin3.github.io/hello/sg/2022/08/21/ODMatrix.html",
            "relUrl": "/sg/2022/08/21/ODMatrix.html",
            "date": " • Aug 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://calamin3.github.io/hello/jupyter/2020/02/21/test-copy.html",
            "relUrl": "/jupyter/2020/02/21/test-copy.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://calamin3.github.io/hello/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://calamin3.github.io/hello/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://calamin3.github.io/hello/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}