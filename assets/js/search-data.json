{
  
    
        "post0": {
            "title": "My take on the Titanic ML Problem",
            "content": "This is my take on machine learning for the iconic Titanic ML dataset. Purpose is not in accuracy of predictions, but rather as a refresher to the different data analysis technique and to the different ML techniques. Will come back from time to time to refresh the techniques used as I become more familiar with data science and machine learning! . Literature . Data was obtained as part of the introductory machine learning &quot;competition&quot; in Kaggle. To say that it is a competition is an overstatement; Rather, it is more of a tutorial to the kaggle ecosystem and submission. In this challenge, the problem statement was to develop a predictive model that answers: &quot;what sorts of people were more likely to survive?&quot; in the Titanic disaster. . Exploratory Data Analysis . import pandas as pd import numpy as np training = pd.read_csv(&#39;train.csv&#39;) test = pd.read_csv(&#39;test.csv&#39;) . training.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . Here are the variables and their types in order of appearances . PassengerId are unique integer values. We will be dropping this column | Survived,Pclass,Sex,Embarked are categorical variables | Names are the names of the passengers. We could be able to perform some feature engineering based on their titles in the names | Age and Fare are continouous values | SibSp and Parch are integer values with discrete counts | Ticket and Cabin are the ticket numbers and cabin numbers. Ticket numbers will be dropped. As for Cabin, We may be able to use the cabin numbers to determine if their cabins of stay are near to emergency lifeboat points. In the Titanic, the first letter of the cabin number indicates the classes, with A Deck belonging to higher class passengers (and nearer to the top deck) and F Deck at the bottom. However, due to the number of missing values (more than 50% missing), I will be dropping Cabin. | . training.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . Age seems to be pretty well distributed among all passengers, with some outliers for Fares. Let us take a look at the distributions of the variables . import seaborn as sns import matplotlib.pyplot as plt . fig,ax = plt.subplots(2,3,figsize=(20,15)) sns.countplot(data=training,x=&#39;Survived&#39;,ax=ax[0][0]) sns.countplot(data=training,x=&#39;Pclass&#39;,hue=&#39;Survived&#39;,ax=ax[0][1]) sns.countplot(data=training,x=&#39;Sex&#39;,hue=&#39;Survived&#39;,ax=ax[0][2]) sns.countplot(data=training,x=&#39;SibSp&#39;,hue=&#39;Survived&#39;,ax=ax[1][0]) sns.countplot(data=training,x=&#39;Parch&#39;,hue=&#39;Survived&#39;,ax=ax[1][1]) sns.countplot(data=training,x=&#39;Embarked&#39;,hue=&#39;Survived&#39;,ax=ax[1][2]) for axes in ax: for a in axes: a.legend(loc=1,title=&#39;Survived&#39;) . No artists with labels found to put in legend. Note that artists whose label start with an underscore are ignored when legend() is called with no argument. . Tried to plot all categorical variables by count, with &#39;Survived&#39; as the hue. From the first plot, majority of people did not survive. Going through the different variables, we can see that following interesting features: . Most people in better Pclass have a higher survival rate (% of survived in Pclass of 1 &gt; 2 &gt; 3) | More females survived than males | Those with SibSp of 1 have the highest survival rate | Those with Parch of 1 have the highest survival rate | Those who embarked from C=Cherburg have a higher survival rate, followed by Q=Queenstown and S=Southampton. FYI, sequence of port calls was from Southampton &gt; Cherburg &gt; Queenstown. | . Let us try to take a look at the continuous features, Age and Fares. . sns.pairplot(data=training[[&#39;Age&#39;,&#39;Survived&#39;,&#39;Fare&#39;]],hue=&#39;Survived&#39;,plot_kws={&#39;alpha&#39;:0.7}) . &lt;seaborn.axisgrid.PairGrid at 0x1ba88c2e400&gt; . training.corr() . PassengerId Survived Pclass Age SibSp Parch Fare . PassengerId 1.000000 | -0.005007 | -0.035144 | 0.036847 | -0.057527 | -0.001652 | 0.012658 | . Survived -0.005007 | 1.000000 | -0.338481 | -0.077221 | -0.035322 | 0.081629 | 0.257307 | . Pclass -0.035144 | -0.338481 | 1.000000 | -0.369226 | 0.083081 | 0.018443 | -0.549500 | . Age 0.036847 | -0.077221 | -0.369226 | 1.000000 | -0.308247 | -0.189119 | 0.096067 | . SibSp -0.057527 | -0.035322 | 0.083081 | -0.308247 | 1.000000 | 0.414838 | 0.159651 | . Parch -0.001652 | 0.081629 | 0.018443 | -0.189119 | 0.414838 | 1.000000 | 0.216225 | . Fare 0.012658 | 0.257307 | -0.549500 | 0.096067 | 0.159651 | 0.216225 | 1.000000 | . Does not seem to have any strong correlations between &#39;Survived&#39;, &#39;Age&#39; and &#39;Fare&#39; . Feature Engineering . We can further improve the model by adding in feature. For example, we could probably include the titles of each person as a feature. We can extract the titles of each person from their name using regular expressions . import re training[&#39;Title_Name&#39;]=training[&#39;Name&#39;].str.extract(r&#39;, s( w+) .&#39;,flags=re.IGNORECASE) training[&#39;Title_Name&#39;].value_counts(dropna=False) #all titles . Mr 517 Miss 182 Mrs 125 Master 40 Dr 7 Rev 6 Mlle 2 Major 2 Col 2 NaN 1 Capt 1 Ms 1 Sir 1 Lady 1 Mme 1 Don 1 Jonkheer 1 Name: Title_Name, dtype: int64 . A quick search shows that some of the more obscure abbreviations are french titles: . Mlle = mademoiselle | Mme = Madame | . We will be replace Mlle with Miss, Mme with Mrs. Also, to minimise the number of categories, we will be grouping all other categories after top 5 categories as &#39;Others&#39; . training[&#39;Title_Name&#39;].replace({&#39;Mlle&#39;:&#39;Miss&#39;,&#39;Mme&#39;:&#39;Mrs&#39;,np.nan:&#39;Others&#39;},inplace=True) replacement = list(training[&#39;Title_Name&#39;].value_counts().index[4:]) new_value = [&#39;Others&#39; for x in replacement] training[&#39;Title_Name&#39;].replace(to_replace=replacement,value=new_value,inplace=True) . test[&#39;Title_Name&#39;]=test[&#39;Name&#39;].str.extract(r&#39;, s( w+) .&#39;,flags=re.IGNORECASE) test[&#39;Title_Name&#39;].replace({&#39;Mlle&#39;:&#39;Miss&#39;,&#39;Mme&#39;:&#39;Mrs&#39;,np.nan:&#39;Others&#39;},inplace=True) replacement = list(test[&#39;Title_Name&#39;].value_counts().index[4:]) new_value = [&#39;Others&#39; for x in replacement] test[&#39;Title_Name&#39;].replace(to_replace=replacement,value=new_value,inplace=True) . Outliers . To remove or not to remove outliers? Firstly, we should differentiate between natural outliers and outilers that happened due to the mistake in the measurement or typographical errors. Of course, if we know that it is due to some mistakes, we should remove the outliers. If there are only a small proportion of outliers, we could also remove them. In this case, the fare outliers does not seem to be a mistake, and the number of outliers is significant. Hence, I will not be removing them. I will however keep the codes here for future use if required. . def remove_outliers(df,column:str,iqr_range=3): &#39;&#39;&#39; Remove outliers based in IQR, with default of any values outside 3 * IQR being removed Returns the indices of the df for which the values in the column is &lt; or &gt; IQR*3 of the 25th and 75th percentile &#39;&#39;&#39; percent_25 = np.percentile(df[column],25) percent_75 = np.percentile(df[column],75) IQR = percent_75 - percent_25 return df.loc[~((df[column]&lt;(percent_25-iqr_range*IQR)) | (df[column]&gt;(percent_75+iqr_range*IQR)))] . Fill NA values . Age and Embarked columns consist of some NA values. We can fill NA values based on the median of the age and the value which appears the most from Embarked. Normally, filling of NA values happens after train-test-split to avoid using testing data, but in this case as the training data is separated, we are able to do it now. . Let&#39;s make use of sklearn SimpleImputer for the filling of NA values . from sklearn.impute import SimpleImputer . imp_median = SimpleImputer(missing_values=np.nan,strategy=&#39;median&#39;,copy=False) imp_freq = SimpleImputer(missing_values=np.nan,strategy=&#39;most_frequent&#39;,copy=False) training[&#39;Age&#39;]=imp_median.fit_transform(training[[&#39;Age&#39;,&#39;Fare&#39;]]) training[&#39;Embarked&#39;]=imp_freq.fit_transform(training[[&#39;Embarked&#39;]]) . Feature selection, scaling and encoding . Before deploying our ML models, we would still be required to perform scaling and encoding of the variables. Categorical variables, such as Sex, would need to be one-hot-encoded as many ML techniques, such as Logistics Regression, are sensitive to such variables. Models employing continouous variables, such as Fares, would also be better off if we scale the data down to scale, either via min-max scaling or Z-score scaling. . We would be performing MinMaxScaling for Age, Fares and OneHotEncoding for Sex,Pclass,Embarked and Title_Name . from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder,MinMaxScaler . features_to_retain = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;,&#39;Embarked&#39;,&#39;Title_Name&#39;] labels = &#39;Survived&#39; . y_train = training[labels] x_train= training[features_to_retain] x_test= test[features_to_retain] . preprocessor = ColumnTransformer( [(&#39;categories&#39;,OneHotEncoder(),[&#39;Pclass&#39;,&#39;Sex&#39;,&#39;Embarked&#39;,&#39;Title_Name&#39;]), (&#39;continuous&#39;,MinMaxScaler(),[&#39;Age&#39;,&#39;Fare&#39;])], remainder=&#39;passthrough&#39; ) . Machine Learning Models . We would run a series of classifiers, with using 10 iterations of RandomSearchCV for efficiency in hyperparameter tunings. The classifiers are: . Naive-Bayes | Logistic Regression | Decision Tree | Random Forest | Adaboost | Gradient Boost | XGBoost | SVM | Neural Network (using tf.keras) | . We would then compare the training metrics for these models as we do not have the testing labels. . from sklearn.pipeline import Pipeline,make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import MultinomialNB from sklearn.model_selection import RandomizedSearchCV from sklearn import metrics,ensemble from sklearn.tree import DecisionTreeClassifier from sklearn import metrics from sklearn import svm from xgboost import XGBClassifier from sklearn.neural_network import MLPClassifier from sklearn.metrics import ConfusionMatrixDisplay from sklearn.metrics import RocCurveDisplay,PrecisionRecallDisplay,precision_recall_curve,average_precision_score . def model_metrics(model,x_train,y_train): y_pred_train = model.predict(x_train) accuracy_train = metrics.accuracy_score(y_train,y_pred_train) f1_train = metrics.f1_score(y_train,y_pred_train) precision_train = metrics.precision_score(y_train,y_pred_train) recall_train = metrics.recall_score(y_train,y_pred_train) roc_auc_train = metrics.roc_auc_score(y_train,model.predict_proba(x_train)[:,1]) ap_score_train = metrics.average_precision_score(y_train,model.predict_proba(x_train)[:,1]) return [accuracy_train,f1_train,precision_train,recall_train,roc_auc_train,ap_score_train] def plot_CM_ROC_PR(x_train,y_train,estimator,name): fig, (ax1,ax2,ax3) = plt.subplots(nrows=1,ncols=3,figsize=(21,6)) cm = ConfusionMatrixDisplay.from_estimator( estimator, x_train, y_train, ax=ax1, cmap=&#39;Reds&#39; # ,display_labels=[&#39;False&#39;,&#39;True&#39;] ) ax1.set_title(f&#39;Confusion Matrix of {name} model:&#39;) RocCurveDisplay.from_estimator(estimator,x_train,y_train,name=name,ax=ax2,color=&#39;orange&#39;) ax2.plot([0, 1], [0, 1], color=&quot;navy&quot;, lw=2, linestyle=&quot;--&quot;) ax2.set_xlim([-0.05,1.05]) ax2.set_ylim([-0.05,1.05]) ax2.set_title(&#39;ROC Curve&#39;) baseline_pr = len(y_train[y_train==1]) / len(y_train) PrecisionRecallDisplay.from_estimator(estimator,x_train,y_train,name=name,ax=ax3,color=&#39;orange&#39;) ax3.plot([0, 1], [baseline_pr, baseline_pr], color=&quot;navy&quot;, lw=2, linestyle=&quot;--&quot;) ax3.set_xlim([-0.05,1.05]) ax3.set_ylim([-0.05,1.05]) ax3.set_title(&#39;Precision Recall Curve&#39;) plt.show() . model_params = { #Multinomial Naive-Bayes &#39;MNB&#39;:{ &#39;model&#39;: MultinomialNB(), &#39;params&#39;: { &#39;MNB__alpha&#39;:[0.01,0.1] } }, # Logistic Regression &#39;LogisticRegression&#39;: { &#39;model&#39;: LogisticRegression(), &#39;params&#39;: { &#39;LogisticRegression__penalty&#39;:[&#39;l1&#39;,&#39;l2&#39;], &#39;LogisticRegression__solver&#39;:[&#39;liblinear&#39;,&#39;saga&#39;,&#39;lbfgs&#39;], &#39;LogisticRegression__C&#39;:[0.0001,0.001,0.01,0.1,1], &#39;LogisticRegression__max_iter&#39;:[1000] } }, # Decision Tree &#39;DT&#39;: { &#39;model&#39;: DecisionTreeClassifier(), &#39;params&#39;: { &#39;DT__criterion&#39;:[&#39;gini&#39;,&#39;entropy&#39;,&#39;log_loss&#39;], &#39;DT__splitter&#39;:[&#39;best&#39;,&#39;random&#39;], &#39;DT__max_depth&#39;:[2,4,6,8,10], &#39;DT__min_samples_split&#39;:[2,4,6], &#39;DT__min_samples_leaf&#39;:[2,4,6], &#39;DT__max_features&#39;:[&#39;sqrt&#39;,&#39;log2&#39;] } }, # Random Forest &#39;RandomForest&#39;: { &#39;model&#39;: ensemble.RandomForestClassifier(), &#39;params&#39;: { &#39;RandomForest__n_estimators&#39;:[50,100,150], &#39;RandomForest__criterion&#39;:[&#39;gini&#39;,&#39;entropy&#39;], &#39;RandomForest__min_samples_split&#39;:[2,4,6,8,10], &#39;RandomForest__min_samples_leaf&#39;:[2,4,6], &#39;RandomForest__max_features&#39;:[&#39;sqrt&#39;,&#39;log2&#39;], &#39;RandomForest__oob_score&#39;:[True] } }, #Ada-boost &#39;Adaboost&#39;:{ &#39;model&#39;:ensemble.AdaBoostClassifier(), &#39;params&#39;: { &#39;Adaboost__base_estimator&#39;:[DecisionTreeClassifier()], &#39;Adaboost__n_estimators&#39;:[50,100,150], &#39;Adaboost__learning_rate&#39;:[0.1,1], &#39;Adaboost__base_estimator__max_depth&#39;:[3,4,5], &#39;Adaboost__algorithm&#39;:[&#39;SAMME.R&#39;,&#39;SAMME&#39;] } }, # GradientBoosting &#39;GradientBoosting&#39;: { &#39;model&#39;: ensemble.GradientBoostingClassifier(), &#39;params&#39;: { &#39;GradientBoosting__loss&#39;:[&#39;log_loss&#39;,&#39;exponential&#39;], &#39;GradientBoosting__learning_rate&#39;:[0.01,0.1,1], &#39;GradientBoosting__n_estimators&#39;:[50,100,150], &#39;GradientBoosting__subsample&#39;:[0.1,0.5,1], &#39;GradientBoosting__min_samples_split&#39;:[2,10,20], &#39;GradientBoosting__min_samples_leaf&#39;:[1,5,10], &#39;GradientBoosting__max_depth&#39;:[3,5,10] } }, # XGBoost &#39;XGBoost&#39;: { &#39;model&#39;: XGBClassifier(), &#39;params&#39;: { &#39;XGBoost__n_estimators&#39;: [int(x) for x in np.linspace(100,2000,5)], &#39;XGBoost__learning_rate&#39;: [0.1, 0.3, 0.5, 0.7, 1.0], &#39;XGBoost__min_child_weight&#39;: [1, 5, 10], &#39;XGBoost__gamma&#39;: [0.5, 1, 1.5, 2, 5], &#39;XGBoost__subsample&#39;: [0.6, 0.8, 1.0], &#39;XGBoost__colsample_bytree&#39;: [0.6, 0.8, 1.0], &#39;XGBoost__max_depth&#39;: [3, 4, 5] } }, # SVM &#39;SVM&#39;: { &#39;model&#39;: svm.SVC(), &#39;params&#39;: { &#39;SVM__probability&#39;:[True], &#39;SVM__C&#39;: [0.001,0.01,0.1,1], &#39;SVM__kernel&#39;: [&#39;linear&#39;, &#39;poly&#39;,&#39;rbf&#39;] } } # # ,MLP # &#39;MLP&#39;: { # &#39;model&#39;: MLPClassifier(), # &#39;params&#39;: { # &#39;MLP__hidden_layer_sizes&#39;:[(32,16),(64,32)], # &#39;MLP__activation&#39;:[&#39;relu&#39;,&#39;logistic&#39;], # &#39;MLP__learning_rate&#39;:[&#39;constant&#39;], # &#39;MLP__learning_rate_init&#39;:[0.001,0.01], # &#39;MLP__early_stopping&#39;:[True] # } # } } . . models_to_tune = list(model_params.keys()) best_models={} results_all = [] for n in range(len(models_to_tune)): pipe = Pipeline( steps=[(&#39;preprocessor&#39;, preprocessor), (models_to_tune[n], model_params[models_to_tune[n]][&#39;model&#39;])]) rs = RandomizedSearchCV(pipe, param_distributions=model_params[models_to_tune[n]][&#39;params&#39;], n_iter=20, scoring=&#39;accuracy&#39;, random_state=2022, verbose=0) print(f&#39;Processing for {models_to_tune[n]}...&#39;) rs.fit(x_train, y_train) best_models[models_to_tune[n]]=rs.best_estimator_ y_pred = rs.predict(x_train) results = [models_to_tune[n]] + model_metrics(rs.best_estimator_,x_train,y_train) results_all.append(results) # tn, fp, fn, tp=metrics.confusion_matrix(y_train,y_pred).ravel() . Processing for MNB... Processing for LogisticRegression... . C: Users Thomas Lim AppData Roaming Python Python39 site-packages sklearn model_selection _search.py:306: UserWarning: The total space of parameters 2 is smaller than n_iter=20. Running 2 iterations. For exhaustive searches, use GridSearchCV. warnings.warn( C: Users Thomas Lim AppData Roaming Python Python39 site-packages sklearn model_selection _validation.py:378: FitFailedWarning: 20 fits failed out of a total of 100. The score on these train-test partitions for these parameters will be set to nan. If these failures are not expected, you can try to debug them by setting error_score=&#39;raise&#39;. Below are more details about the failures: -- 20 fits failed with the following error: Traceback (most recent call last): File &#34;C: Users Thomas Lim AppData Roaming Python Python39 site-packages sklearn model_selection _validation.py&#34;, line 686, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &#34;C: Users Thomas Lim AppData Roaming Python Python39 site-packages sklearn pipeline.py&#34;, line 382, in fit self._final_estimator.fit(Xt, y, **fit_params_last_step) File &#34;C: Users Thomas Lim AppData Roaming Python Python39 site-packages sklearn linear_model _logistic.py&#34;, line 1091, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &#34;C: Users Thomas Lim AppData Roaming Python Python39 site-packages sklearn linear_model _logistic.py&#34;, line 61, in _check_solver raise ValueError( ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. warnings.warn(some_fits_failed_message, FitFailedWarning) C: Users Thomas Lim AppData Roaming Python Python39 site-packages sklearn model_selection _search.py:953: UserWarning: One or more of the test scores are non-finite: [0.61616345 0.61616345 0.61616345 nan nan 0.8271546 0.61616345 0.81591865 nan 0.8002197 0.61616345 0.80356538 0.82603101 0.61616345 0.64311092 0.61616345 0.81704224 nan 0.61616345 0.61616345] warnings.warn( . Processing for DT... Processing for RandomForest... Processing for Adaboost... Processing for GradientBoosting... Processing for XGBoost... Processing for SVM... . C: Users Thomas Lim AppData Roaming Python Python39 site-packages sklearn model_selection _search.py:306: UserWarning: The total space of parameters 12 is smaller than n_iter=20. Running 12 iterations. For exhaustive searches, use GridSearchCV. warnings.warn( . . Using tfkeras for Neural Network (MLP) . import tensorflow as tf import seaborn as sns . def create_model(my_learning_rate,layers=[20,12]): model = tf.keras.models.Sequential() #hidden layers for index,layer in enumerate(layers): model.add(tf.keras.layers.Dense(units=layer,activation=&#39;relu&#39;,name = f&#39;hidden{index}&#39;)) #output layer model.add(tf.keras.layers.Dense(units=1,activation=&#39;sigmoid&#39;,name=&#39;output&#39;)) model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate), loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;,&#39;AUC&#39;,&#39;Precision&#39;,&#39;Recall&#39;]) return model def train_model(model,x_train_normalised,y_train,epochs,batch_size,validation_split,cb_list): history = model.fit(x=x_train_normalised, y=y_train, batch_size=batch_size, epochs=epochs, shuffle=True, validation_split=validation_split,callbacks=cb_list) return history def plot_curve(hist): &quot;&quot;&quot;Plot a curve of one or more classification metrics vs. epoch.&quot;&quot;&quot; # list_of_metrics should be one of the names shown in: # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics epochs = hist.epoch f, ax = plt.subplots(ncols=2, figsize=(20,12)) ax[0].plot(epochs, hist.history[&#39;accuracy&#39;], label=&#39;Training Accuracy&#39;) ax[0].plot(epochs, hist.history[&#39;val_accuracy&#39;], label=&#39;Validation Accuracy&#39;) ax[0].set_xlabel(&#39;Epochs&#39;) ax[0].set_ylabel(&#39;Accuracy&#39;) ax[0].legend() ax[0].set_xticks(epochs) ax[1].plot(epochs, hist.history[&#39;loss&#39;], label=&#39;Training Loss&#39;) ax[1].plot(epochs, hist.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;) ax[1].set_xlabel(&#39;Epochs&#39;) ax[1].set_ylabel(&#39;Loss&#39;) ax[1].legend() ax[1].set_xticks(epochs) plt.show() def plot_CM_ROC_PR_NN(x_train,y_train,my_model,name): fig, (ax1,ax2,ax3) = plt.subplots(nrows=1,ncols=3,figsize=(21,6)) if name==&#39;NN&#39;: probs = my_model.predict(x_train) #use this for NN as estimator.predict for sklearn gives us the actual classes else: probs = my_model.predict_proba(x_train) y_pred = probs &gt; 0.5 #default threshold at 0.5 ConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=ax1,cmap=&#39;Reds&#39;) ax1.set_title(f&#39;Confusion Matrix of {name} model:&#39;) fpr,tpr,_ = metrics.roc_curve(y_train,probs) roc_auc = metrics.auc(fpr, tpr) ax2.plot(fpr, tpr,color=&#39;orange&#39;) ax2.plot([0, 1], [0, 1], color=&quot;navy&quot;, lw=2, linestyle=&quot;--&quot;) ax2.legend([f&#39;NN (AUC = {roc_auc:.2f})&#39;],loc=&#39;lower right&#39;) ax2.set(xlabel=&#39;False Positive Rate&#39;,ylabel=&#39;True Positive Rate&#39;) ax2.set_xlim([-0.05,1.05]) ax2.set_ylim([-0.05,1.05]) ax2.set_title(&#39;ROC Curve&#39;) baseline_pr = len(y_train[y_train==1]) / len(y_train) average_precision = average_precision_score(y_train, probs) precision, recall, _ = precision_recall_curve(y_train, probs) ax3.plot(recall, precision, color=&#39;orange&#39;) ax3.plot([0, 1], [baseline_pr, baseline_pr], color=&quot;navy&quot;, lw=2, linestyle=&quot;--&quot;) ax2.set(xlabel=&#39;Recall&#39;,ylabel=&#39;Precision&#39;) ax3.legend([f&#39;NN (AP = {average_precision:.2f})&#39;],loc=&#39;lower right&#39;) ax3.set_xlim([-0.05,1.05]) ax3.set_ylim([-0.05,1.05]) ax3.set_title(&#39;Precision-Recall Curve&#39;) def model_metrics_nn(probs,y_train,y_pred): #for NN models using keras accuracy_train = metrics.accuracy_score(y_train,y_pred) f1_train = metrics.f1_score(y_train,y_pred) precision_train = metrics.precision_score(y_train,y_pred) recall_train = metrics.recall_score(y_train,y_pred) roc_auc_train = metrics.roc_auc_score(y_train,probs) ap_score_train = metrics.average_precision_score(y_train,probs) return [accuracy_train,f1_train,precision_train,recall_train,roc_auc_train,ap_score_train] . #normalise/scaling pipe2 = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor)]) x_train_nn = pipe2.fit_transform(x_train) y_train_nn = y_train.values #hyperparameters to use learning_rate = 0.003 epochs = 100 batch_size = 200 validation_split = 0.3 #set early stop es = tf.keras.callbacks.EarlyStopping(monitor=&quot;val_loss&quot;,min_delta=0,patience=10,verbose=1,mode=&quot;auto&quot;,baseline=None,restore_best_weights=False,) mc = tf.keras.callbacks.ModelCheckpoint(&quot;best_model.h5&quot;,monitor=&quot;val_accuracy&quot;,mode=&quot;auto&quot;,verbose=1,save_best_only=True,) #creating model my_model = create_model(learning_rate) #fitting/training model hist = train_model(my_model,x_train_nn,y_train_nn,epochs,batch_size,validation_split,[es,mc]) #load the best model from early stoppage my_model = tf.keras.models.load_model(&#39;best_model.h5&#39;) . Epoch 1/100 1/4 [======&gt;.......................] - ETA: 2s - loss: 0.7261 - accuracy: 0.3700 - auc: 0.3969 - precision: 0.2885 - recall: 0.3659 Epoch 1: val_accuracy improved from -inf to 0.64179, saving model to best_model.h5 4/4 [==============================] - 1s 114ms/step - loss: 0.7107 - accuracy: 0.4061 - auc: 0.4081 - precision: 0.2862 - recall: 0.3374 - val_loss: 0.6751 - val_accuracy: 0.6418 - val_auc: 0.5224 - val_precision: 0.5000 - val_recall: 0.2500 Epoch 2/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.6645 - accuracy: 0.6450 - auc: 0.5969 - precision: 0.5625 - recall: 0.2400 Epoch 2: val_accuracy improved from 0.64179 to 0.68657, saving model to best_model.h5 4/4 [==============================] - 0s 17ms/step - loss: 0.6633 - accuracy: 0.6308 - auc: 0.6016 - precision: 0.5909 - recall: 0.2114 - val_loss: 0.6339 - val_accuracy: 0.6866 - val_auc: 0.6972 - val_precision: 0.6765 - val_recall: 0.2396 Epoch 3/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.6452 - accuracy: 0.6200 - auc: 0.7585 - precision: 0.7692 - recall: 0.2222 Epoch 3: val_accuracy improved from 0.68657 to 0.72015, saving model to best_model.h5 4/4 [==============================] - 0s 14ms/step - loss: 0.6265 - accuracy: 0.6758 - auc: 0.7771 - precision: 0.8438 - recall: 0.2195 - val_loss: 0.5962 - val_accuracy: 0.7201 - val_auc: 0.7984 - val_precision: 0.9200 - val_recall: 0.2396 Epoch 4/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.6009 - accuracy: 0.7000 - auc: 0.8331 - precision: 1.0000 - recall: 0.2683 Epoch 4: val_accuracy improved from 0.72015 to 0.72388, saving model to best_model.h5 4/4 [==============================] - 0s 15ms/step - loss: 0.5932 - accuracy: 0.6966 - auc: 0.8281 - precision: 0.9254 - recall: 0.2520 - val_loss: 0.5635 - val_accuracy: 0.7239 - val_auc: 0.8216 - val_precision: 0.9231 - val_recall: 0.2500 Epoch 5/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.5718 - accuracy: 0.7400 - auc: 0.8324 - precision: 0.8857 - recall: 0.3924 Epoch 5: val_accuracy improved from 0.72388 to 0.73134, saving model to best_model.h5 4/4 [==============================] - 0s 13ms/step - loss: 0.5642 - accuracy: 0.7191 - auc: 0.8400 - precision: 0.9176 - recall: 0.3171 - val_loss: 0.5365 - val_accuracy: 0.7313 - val_auc: 0.8279 - val_precision: 0.9286 - val_recall: 0.2708 Epoch 6/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.5287 - accuracy: 0.7400 - auc: 0.8867 - precision: 0.9189 - recall: 0.4096 Epoch 6: val_accuracy improved from 0.73134 to 0.79104, saving model to best_model.h5 4/4 [==============================] - 0s 13ms/step - loss: 0.5407 - accuracy: 0.7352 - auc: 0.8448 - precision: 0.9010 - recall: 0.3699 - val_loss: 0.5151 - val_accuracy: 0.7910 - val_auc: 0.8341 - val_precision: 0.9167 - val_recall: 0.4583 Epoch 7/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.5226 - accuracy: 0.7850 - auc: 0.8588 - precision: 0.9286 - recall: 0.4937 Epoch 7: val_accuracy improved from 0.79104 to 0.80224, saving model to best_model.h5 4/4 [==============================] - 0s 14ms/step - loss: 0.5198 - accuracy: 0.7881 - auc: 0.8491 - precision: 0.8701 - recall: 0.5447 - val_loss: 0.4962 - val_accuracy: 0.8022 - val_auc: 0.8424 - val_precision: 0.7945 - val_recall: 0.6042 Epoch 8/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4973 - accuracy: 0.8450 - auc: 0.8634 - precision: 0.9016 - recall: 0.6875 Epoch 8: val_accuracy improved from 0.80224 to 0.80597, saving model to best_model.h5 4/4 [==============================] - 0s 12ms/step - loss: 0.5022 - accuracy: 0.8266 - auc: 0.8532 - precision: 0.8450 - recall: 0.6870 - val_loss: 0.4799 - val_accuracy: 0.8060 - val_auc: 0.8529 - val_precision: 0.7821 - val_recall: 0.6354 Epoch 9/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.5091 - accuracy: 0.8150 - auc: 0.8655 - precision: 0.8718 - recall: 0.7158 Epoch 9: val_accuracy did not improve from 0.80597 4/4 [==============================] - 0s 7ms/step - loss: 0.4880 - accuracy: 0.8106 - auc: 0.8535 - precision: 0.7963 - recall: 0.6992 - val_loss: 0.4662 - val_accuracy: 0.8022 - val_auc: 0.8646 - val_precision: 0.7654 - val_recall: 0.6458 Epoch 10/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4699 - accuracy: 0.8100 - auc: 0.8809 - precision: 0.8052 - recall: 0.7294 Epoch 10: val_accuracy did not improve from 0.80597 4/4 [==============================] - 0s 7ms/step - loss: 0.4754 - accuracy: 0.8090 - auc: 0.8557 - precision: 0.7848 - recall: 0.7114 - val_loss: 0.4534 - val_accuracy: 0.8022 - val_auc: 0.8656 - val_precision: 0.7590 - val_recall: 0.6562 Epoch 11/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4285 - accuracy: 0.8450 - auc: 0.8705 - precision: 0.8261 - recall: 0.7500 Epoch 11: val_accuracy did not improve from 0.80597 4/4 [==============================] - 0s 7ms/step - loss: 0.4648 - accuracy: 0.8106 - auc: 0.8551 - precision: 0.7857 - recall: 0.7154 - val_loss: 0.4433 - val_accuracy: 0.8060 - val_auc: 0.8655 - val_precision: 0.7619 - val_recall: 0.6667 Epoch 12/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4903 - accuracy: 0.7700 - auc: 0.8554 - precision: 0.7746 - recall: 0.6471 Epoch 12: val_accuracy improved from 0.80597 to 0.81343, saving model to best_model.h5 4/4 [==============================] - 0s 13ms/step - loss: 0.4559 - accuracy: 0.8106 - auc: 0.8569 - precision: 0.7857 - recall: 0.7154 - val_loss: 0.4347 - val_accuracy: 0.8134 - val_auc: 0.8683 - val_precision: 0.7674 - val_recall: 0.6875 Epoch 13/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4445 - accuracy: 0.8250 - auc: 0.8627 - precision: 0.8310 - recall: 0.7195 Epoch 13: val_accuracy did not improve from 0.81343 4/4 [==============================] - 0s 7ms/step - loss: 0.4487 - accuracy: 0.8138 - auc: 0.8589 - precision: 0.7876 - recall: 0.7236 - val_loss: 0.4268 - val_accuracy: 0.8134 - val_auc: 0.8757 - val_precision: 0.7674 - val_recall: 0.6875 Epoch 14/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4771 - accuracy: 0.7950 - auc: 0.8286 - precision: 0.7612 - recall: 0.6711 Epoch 14: val_accuracy did not improve from 0.81343 4/4 [==============================] - 0s 7ms/step - loss: 0.4413 - accuracy: 0.8154 - auc: 0.8643 - precision: 0.7885 - recall: 0.7276 - val_loss: 0.4200 - val_accuracy: 0.8134 - val_auc: 0.8801 - val_precision: 0.7674 - val_recall: 0.6875 Epoch 15/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4010 - accuracy: 0.8300 - auc: 0.9130 - precision: 0.8507 - recall: 0.7037 Epoch 15: val_accuracy improved from 0.81343 to 0.82090, saving model to best_model.h5 4/4 [==============================] - 0s 13ms/step - loss: 0.4354 - accuracy: 0.8170 - auc: 0.8662 - precision: 0.7895 - recall: 0.7317 - val_loss: 0.4137 - val_accuracy: 0.8209 - val_auc: 0.8788 - val_precision: 0.7857 - val_recall: 0.6875 Epoch 16/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4507 - accuracy: 0.8150 - auc: 0.8567 - precision: 0.7941 - recall: 0.7013 Epoch 16: val_accuracy did not improve from 0.82090 4/4 [==============================] - 0s 7ms/step - loss: 0.4310 - accuracy: 0.8170 - auc: 0.8677 - precision: 0.7870 - recall: 0.7358 - val_loss: 0.4100 - val_accuracy: 0.8209 - val_auc: 0.8831 - val_precision: 0.7727 - val_recall: 0.7083 Epoch 17/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4746 - accuracy: 0.7850 - auc: 0.8488 - precision: 0.7763 - recall: 0.6941 Epoch 17: val_accuracy improved from 0.82090 to 0.83209, saving model to best_model.h5 4/4 [==============================] - 0s 12ms/step - loss: 0.4279 - accuracy: 0.8186 - auc: 0.8673 - precision: 0.7854 - recall: 0.7439 - val_loss: 0.4041 - val_accuracy: 0.8321 - val_auc: 0.8829 - val_precision: 0.7865 - val_recall: 0.7292 Epoch 18/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4109 - accuracy: 0.8250 - auc: 0.8874 - precision: 0.8148 - recall: 0.7674 Epoch 18: val_accuracy did not improve from 0.83209 4/4 [==============================] - 0s 7ms/step - loss: 0.4243 - accuracy: 0.8202 - auc: 0.8685 - precision: 0.7888 - recall: 0.7439 - val_loss: 0.3974 - val_accuracy: 0.8284 - val_auc: 0.8849 - val_precision: 0.7907 - val_recall: 0.7083 Epoch 19/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3947 - accuracy: 0.8300 - auc: 0.8896 - precision: 0.7792 - recall: 0.7792 Epoch 19: val_accuracy improved from 0.83209 to 0.83955, saving model to best_model.h5 4/4 [==============================] - 0s 12ms/step - loss: 0.4217 - accuracy: 0.8186 - auc: 0.8652 - precision: 0.7930 - recall: 0.7317 - val_loss: 0.3935 - val_accuracy: 0.8396 - val_auc: 0.8850 - val_precision: 0.8272 - val_recall: 0.6979 Epoch 20/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3998 - accuracy: 0.8200 - auc: 0.8848 - precision: 0.7564 - recall: 0.7763 Epoch 20: val_accuracy did not improve from 0.83955 4/4 [==============================] - 0s 8ms/step - loss: 0.4186 - accuracy: 0.8202 - auc: 0.8661 - precision: 0.7965 - recall: 0.7317 - val_loss: 0.3923 - val_accuracy: 0.8358 - val_auc: 0.8820 - val_precision: 0.8171 - val_recall: 0.6979 Epoch 21/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4003 - accuracy: 0.8500 - auc: 0.8831 - precision: 0.8684 - recall: 0.7674 Epoch 21: val_accuracy did not improve from 0.83955 4/4 [==============================] - 0s 8ms/step - loss: 0.4171 - accuracy: 0.8218 - auc: 0.8671 - precision: 0.7974 - recall: 0.7358 - val_loss: 0.3921 - val_accuracy: 0.8396 - val_auc: 0.8802 - val_precision: 0.8193 - val_recall: 0.7083 Epoch 22/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3705 - accuracy: 0.8500 - auc: 0.9071 - precision: 0.8684 - recall: 0.7674 Epoch 22: val_accuracy did not improve from 0.83955 4/4 [==============================] - 0s 8ms/step - loss: 0.4153 - accuracy: 0.8218 - auc: 0.8682 - precision: 0.7974 - recall: 0.7358 - val_loss: 0.3893 - val_accuracy: 0.8396 - val_auc: 0.8814 - val_precision: 0.8193 - val_recall: 0.7083 Epoch 23/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3338 - accuracy: 0.8700 - auc: 0.9250 - precision: 0.8133 - recall: 0.8356 Epoch 23: val_accuracy did not improve from 0.83955 4/4 [==============================] - 0s 7ms/step - loss: 0.4135 - accuracy: 0.8218 - auc: 0.8673 - precision: 0.7974 - recall: 0.7358 - val_loss: 0.3865 - val_accuracy: 0.8396 - val_auc: 0.8856 - val_precision: 0.8193 - val_recall: 0.7083 Epoch 24/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3871 - accuracy: 0.8300 - auc: 0.8764 - precision: 0.8000 - recall: 0.7368 Epoch 24: val_accuracy did not improve from 0.83955 4/4 [==============================] - 0s 8ms/step - loss: 0.4117 - accuracy: 0.8266 - auc: 0.8710 - precision: 0.8136 - recall: 0.7276 - val_loss: 0.3863 - val_accuracy: 0.8358 - val_auc: 0.8857 - val_precision: 0.8171 - val_recall: 0.6979 Epoch 25/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4082 - accuracy: 0.8400 - auc: 0.8879 - precision: 0.8667 - recall: 0.7471 Epoch 25: val_accuracy did not improve from 0.83955 4/4 [==============================] - 0s 9ms/step - loss: 0.4099 - accuracy: 0.8250 - auc: 0.8738 - precision: 0.8100 - recall: 0.7276 - val_loss: 0.3873 - val_accuracy: 0.8396 - val_auc: 0.8865 - val_precision: 0.8272 - val_recall: 0.6979 Epoch 26/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4270 - accuracy: 0.8150 - auc: 0.8510 - precision: 0.7703 - recall: 0.7403 Epoch 26: val_accuracy improved from 0.83955 to 0.84328, saving model to best_model.h5 4/4 [==============================] - 0s 15ms/step - loss: 0.4086 - accuracy: 0.8331 - auc: 0.8716 - precision: 0.8198 - recall: 0.7398 - val_loss: 0.3866 - val_accuracy: 0.8433 - val_auc: 0.8866 - val_precision: 0.8293 - val_recall: 0.7083 Epoch 27/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3917 - accuracy: 0.8400 - auc: 0.8705 - precision: 0.7826 - recall: 0.7606 Epoch 27: val_accuracy did not improve from 0.84328 4/4 [==============================] - 0s 9ms/step - loss: 0.4064 - accuracy: 0.8347 - auc: 0.8727 - precision: 0.8235 - recall: 0.7398 - val_loss: 0.3854 - val_accuracy: 0.8433 - val_auc: 0.8869 - val_precision: 0.8214 - val_recall: 0.7188 Epoch 28/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4157 - accuracy: 0.8300 - auc: 0.8699 - precision: 0.8272 - recall: 0.7701 Epoch 28: val_accuracy did not improve from 0.84328 4/4 [==============================] - 0s 8ms/step - loss: 0.4059 - accuracy: 0.8331 - auc: 0.8739 - precision: 0.8170 - recall: 0.7439 - val_loss: 0.3833 - val_accuracy: 0.8433 - val_auc: 0.8866 - val_precision: 0.8214 - val_recall: 0.7188 Epoch 29/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4117 - accuracy: 0.8250 - auc: 0.8724 - precision: 0.8205 - recall: 0.7529 Epoch 29: val_accuracy improved from 0.84328 to 0.84701, saving model to best_model.h5 4/4 [==============================] - 0s 11ms/step - loss: 0.4044 - accuracy: 0.8363 - auc: 0.8759 - precision: 0.8429 - recall: 0.7195 - val_loss: 0.3798 - val_accuracy: 0.8470 - val_auc: 0.8898 - val_precision: 0.8667 - val_recall: 0.6771 Epoch 30/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3573 - accuracy: 0.8650 - auc: 0.8766 - precision: 0.9038 - recall: 0.6812 Epoch 30: val_accuracy did not improve from 0.84701 4/4 [==============================] - 0s 8ms/step - loss: 0.4045 - accuracy: 0.8347 - auc: 0.8751 - precision: 0.8865 - recall: 0.6667 - val_loss: 0.3792 - val_accuracy: 0.8470 - val_auc: 0.8898 - val_precision: 0.8767 - val_recall: 0.6667 Epoch 31/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3568 - accuracy: 0.8650 - auc: 0.8996 - precision: 0.9153 - recall: 0.7105 Epoch 31: val_accuracy did not improve from 0.84701 4/4 [==============================] - 0s 8ms/step - loss: 0.4032 - accuracy: 0.8363 - auc: 0.8777 - precision: 0.8750 - recall: 0.6829 - val_loss: 0.3800 - val_accuracy: 0.8433 - val_auc: 0.8922 - val_precision: 0.8649 - val_recall: 0.6667 Epoch 32/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3706 - accuracy: 0.8650 - auc: 0.9081 - precision: 0.8533 - recall: 0.8000 Epoch 32: val_accuracy improved from 0.84701 to 0.85075, saving model to best_model.h5 4/4 [==============================] - 0s 13ms/step - loss: 0.4018 - accuracy: 0.8395 - auc: 0.8774 - precision: 0.8650 - recall: 0.7033 - val_loss: 0.3812 - val_accuracy: 0.8507 - val_auc: 0.8900 - val_precision: 0.8684 - val_recall: 0.6875 Epoch 33/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4416 - accuracy: 0.8250 - auc: 0.8441 - precision: 0.8036 - recall: 0.6522 Epoch 33: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 9ms/step - loss: 0.4011 - accuracy: 0.8395 - auc: 0.8748 - precision: 0.8650 - recall: 0.7033 - val_loss: 0.3828 - val_accuracy: 0.8507 - val_auc: 0.8882 - val_precision: 0.8684 - val_recall: 0.6875 Epoch 34/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4204 - accuracy: 0.8150 - auc: 0.8631 - precision: 0.8281 - recall: 0.6709 Epoch 34: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 8ms/step - loss: 0.4008 - accuracy: 0.8379 - auc: 0.8755 - precision: 0.8607 - recall: 0.7033 - val_loss: 0.3827 - val_accuracy: 0.8507 - val_auc: 0.8906 - val_precision: 0.8684 - val_recall: 0.6875 Epoch 35/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4015 - accuracy: 0.8400 - auc: 0.8741 - precision: 0.8000 - recall: 0.7059 Epoch 35: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 8ms/step - loss: 0.3986 - accuracy: 0.8379 - auc: 0.8787 - precision: 0.8607 - recall: 0.7033 - val_loss: 0.3818 - val_accuracy: 0.8470 - val_auc: 0.8900 - val_precision: 0.8571 - val_recall: 0.6875 Epoch 36/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3961 - accuracy: 0.8350 - auc: 0.8900 - precision: 0.8500 - recall: 0.7640 Epoch 36: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 8ms/step - loss: 0.4002 - accuracy: 0.8331 - auc: 0.8766 - precision: 0.8349 - recall: 0.7195 - val_loss: 0.3830 - val_accuracy: 0.8358 - val_auc: 0.8910 - val_precision: 0.8250 - val_recall: 0.6875 Epoch 37/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4495 - accuracy: 0.8200 - auc: 0.8464 - precision: 0.8028 - recall: 0.7215 Epoch 37: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 7ms/step - loss: 0.4000 - accuracy: 0.8347 - auc: 0.8794 - precision: 0.8326 - recall: 0.7276 - val_loss: 0.3795 - val_accuracy: 0.8358 - val_auc: 0.8924 - val_precision: 0.8250 - val_recall: 0.6875 Epoch 38/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3977 - accuracy: 0.8450 - auc: 0.8615 - precision: 0.8226 - recall: 0.7183 Epoch 38: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 7ms/step - loss: 0.3983 - accuracy: 0.8411 - auc: 0.8796 - precision: 0.8585 - recall: 0.7154 - val_loss: 0.3762 - val_accuracy: 0.8507 - val_auc: 0.8957 - val_precision: 0.8784 - val_recall: 0.6771 Epoch 39/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3962 - accuracy: 0.8350 - auc: 0.8833 - precision: 0.8871 - recall: 0.6790 Epoch 39: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 7ms/step - loss: 0.3972 - accuracy: 0.8379 - auc: 0.8789 - precision: 0.8796 - recall: 0.6829 - val_loss: 0.3750 - val_accuracy: 0.8507 - val_auc: 0.8952 - val_precision: 0.8784 - val_recall: 0.6771 Epoch 40/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3698 - accuracy: 0.8550 - auc: 0.9086 - precision: 0.8226 - recall: 0.7391 Epoch 40: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 7ms/step - loss: 0.3973 - accuracy: 0.8379 - auc: 0.8785 - precision: 0.8836 - recall: 0.6789 - val_loss: 0.3759 - val_accuracy: 0.8470 - val_auc: 0.8947 - val_precision: 0.8667 - val_recall: 0.6771 Epoch 41/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4326 - accuracy: 0.8150 - auc: 0.8545 - precision: 0.8529 - recall: 0.6824 Epoch 41: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 7ms/step - loss: 0.3974 - accuracy: 0.8379 - auc: 0.8793 - precision: 0.8680 - recall: 0.6951 - val_loss: 0.3815 - val_accuracy: 0.8358 - val_auc: 0.8901 - val_precision: 0.8333 - val_recall: 0.6771 Epoch 42/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3689 - accuracy: 0.8700 - auc: 0.8781 - precision: 0.9091 - recall: 0.7500 Epoch 42: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 7ms/step - loss: 0.3965 - accuracy: 0.8363 - auc: 0.8787 - precision: 0.8429 - recall: 0.7195 - val_loss: 0.3824 - val_accuracy: 0.8396 - val_auc: 0.8904 - val_precision: 0.8442 - val_recall: 0.6771 Epoch 43/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3858 - accuracy: 0.8450 - auc: 0.8764 - precision: 0.8406 - recall: 0.7436 Epoch 43: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 7ms/step - loss: 0.3957 - accuracy: 0.8363 - auc: 0.8793 - precision: 0.8495 - recall: 0.7114 - val_loss: 0.3806 - val_accuracy: 0.8470 - val_auc: 0.8899 - val_precision: 0.8571 - val_recall: 0.6875 Epoch 44/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3563 - accuracy: 0.8650 - auc: 0.9040 - precision: 0.8507 - recall: 0.7703 Epoch 44: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 7ms/step - loss: 0.3947 - accuracy: 0.8363 - auc: 0.8789 - precision: 0.8564 - recall: 0.7033 - val_loss: 0.3816 - val_accuracy: 0.8470 - val_auc: 0.8893 - val_precision: 0.8571 - val_recall: 0.6875 Epoch 45/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3653 - accuracy: 0.8400 - auc: 0.9038 - precision: 0.8529 - recall: 0.7250 Epoch 45: val_accuracy did not improve from 0.85075 4/4 [==============================] - 0s 7ms/step - loss: 0.3941 - accuracy: 0.8363 - auc: 0.8804 - precision: 0.8529 - recall: 0.7073 - val_loss: 0.3813 - val_accuracy: 0.8507 - val_auc: 0.8919 - val_precision: 0.8684 - val_recall: 0.6875 Epoch 46/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3389 - accuracy: 0.8600 - auc: 0.9155 - precision: 0.8784 - recall: 0.7738 Epoch 46: val_accuracy improved from 0.85075 to 0.85448, saving model to best_model.h5 4/4 [==============================] - 0s 12ms/step - loss: 0.3928 - accuracy: 0.8347 - auc: 0.8815 - precision: 0.8522 - recall: 0.7033 - val_loss: 0.3791 - val_accuracy: 0.8545 - val_auc: 0.8951 - val_precision: 0.8800 - val_recall: 0.6875 Epoch 47/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4028 - accuracy: 0.8300 - auc: 0.8718 - precision: 0.8667 - recall: 0.6667 Epoch 47: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 8ms/step - loss: 0.3917 - accuracy: 0.8379 - auc: 0.8804 - precision: 0.8718 - recall: 0.6911 - val_loss: 0.3766 - val_accuracy: 0.8545 - val_auc: 0.8961 - val_precision: 0.8904 - val_recall: 0.6771 Epoch 48/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4265 - accuracy: 0.8300 - auc: 0.8651 - precision: 0.8475 - recall: 0.6667 Epoch 48: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 8ms/step - loss: 0.3927 - accuracy: 0.8347 - auc: 0.8792 - precision: 0.8950 - recall: 0.6585 - val_loss: 0.3759 - val_accuracy: 0.8470 - val_auc: 0.8983 - val_precision: 0.8873 - val_recall: 0.6562 Epoch 49/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3704 - accuracy: 0.8550 - auc: 0.8783 - precision: 0.9286 - recall: 0.6753 Epoch 49: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 8ms/step - loss: 0.3933 - accuracy: 0.8379 - auc: 0.8800 - precision: 0.9096 - recall: 0.6545 - val_loss: 0.3744 - val_accuracy: 0.8507 - val_auc: 0.8973 - val_precision: 0.8889 - val_recall: 0.6667 Epoch 50/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4077 - accuracy: 0.8300 - auc: 0.8748 - precision: 0.8833 - recall: 0.6625 Epoch 50: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 8ms/step - loss: 0.3908 - accuracy: 0.8363 - auc: 0.8804 - precision: 0.9045 - recall: 0.6545 - val_loss: 0.3755 - val_accuracy: 0.8545 - val_auc: 0.8940 - val_precision: 0.8904 - val_recall: 0.6771 Epoch 51/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3242 - accuracy: 0.8800 - auc: 0.9094 - precision: 0.9322 - recall: 0.7333 Epoch 51: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 7ms/step - loss: 0.3905 - accuracy: 0.8395 - auc: 0.8808 - precision: 0.8925 - recall: 0.6748 - val_loss: 0.3779 - val_accuracy: 0.8507 - val_auc: 0.8926 - val_precision: 0.8784 - val_recall: 0.6771 Epoch 52/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3710 - accuracy: 0.8550 - auc: 0.8875 - precision: 0.9444 - recall: 0.6623 Epoch 52: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 8ms/step - loss: 0.3896 - accuracy: 0.8411 - auc: 0.8826 - precision: 0.8731 - recall: 0.6992 - val_loss: 0.3807 - val_accuracy: 0.8470 - val_auc: 0.8893 - val_precision: 0.8667 - val_recall: 0.6771 Epoch 53/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3592 - accuracy: 0.8600 - auc: 0.8952 - precision: 0.9275 - recall: 0.7356 Epoch 53: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 7ms/step - loss: 0.3914 - accuracy: 0.8379 - auc: 0.8828 - precision: 0.8571 - recall: 0.7073 - val_loss: 0.3880 - val_accuracy: 0.8433 - val_auc: 0.8863 - val_precision: 0.8462 - val_recall: 0.6875 Epoch 54/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4020 - accuracy: 0.8300 - auc: 0.8715 - precision: 0.8235 - recall: 0.7179 Epoch 54: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 8ms/step - loss: 0.3958 - accuracy: 0.8379 - auc: 0.8814 - precision: 0.8436 - recall: 0.7236 - val_loss: 0.3970 - val_accuracy: 0.8321 - val_auc: 0.8818 - val_precision: 0.8148 - val_recall: 0.6875 Epoch 55/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3463 - accuracy: 0.8550 - auc: 0.8966 - precision: 0.8750 - recall: 0.7590 Epoch 55: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 7ms/step - loss: 0.3956 - accuracy: 0.8363 - auc: 0.8812 - precision: 0.8364 - recall: 0.7276 - val_loss: 0.3867 - val_accuracy: 0.8433 - val_auc: 0.8820 - val_precision: 0.8553 - val_recall: 0.6771 Epoch 56/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4557 - accuracy: 0.8100 - auc: 0.8382 - precision: 0.7846 - recall: 0.6800 Epoch 56: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 8ms/step - loss: 0.3915 - accuracy: 0.8379 - auc: 0.8844 - precision: 0.8607 - recall: 0.7033 - val_loss: 0.3873 - val_accuracy: 0.8470 - val_auc: 0.8782 - val_precision: 0.8873 - val_recall: 0.6562 Epoch 57/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3907 - accuracy: 0.8400 - auc: 0.8942 - precision: 0.9167 - recall: 0.6707 Epoch 57: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 8ms/step - loss: 0.3946 - accuracy: 0.8395 - auc: 0.8829 - precision: 0.9101 - recall: 0.6585 - val_loss: 0.3884 - val_accuracy: 0.8470 - val_auc: 0.8786 - val_precision: 0.8873 - val_recall: 0.6562 Epoch 58/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.4103 - accuracy: 0.8350 - auc: 0.8746 - precision: 0.9074 - recall: 0.6364 Epoch 58: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 7ms/step - loss: 0.3933 - accuracy: 0.8411 - auc: 0.8834 - precision: 0.9106 - recall: 0.6626 - val_loss: 0.3871 - val_accuracy: 0.8470 - val_auc: 0.8788 - val_precision: 0.8873 - val_recall: 0.6562 Epoch 59/100 1/4 [======&gt;.......................] - ETA: 0s - loss: 0.3434 - accuracy: 0.8650 - auc: 0.9096 - precision: 0.8833 - recall: 0.7260 Epoch 59: val_accuracy did not improve from 0.85448 4/4 [==============================] - 0s 7ms/step - loss: 0.3898 - accuracy: 0.8411 - auc: 0.8853 - precision: 0.9016 - recall: 0.6707 - val_loss: 0.3859 - val_accuracy: 0.8470 - val_auc: 0.8816 - val_precision: 0.8873 - val_recall: 0.6562 Epoch 59: early stopping . . plot_curve(hist) . . probs = my_model.predict(x_train_nn) y_pred = np.where(probs &gt; 0.5,1,0).flatten() nn_metrics = model_metrics_nn(probs,y_train,y_pred) results_all.append([&#39;NN&#39;]+nn_metrics) . 28/28 [==============================] - 0s 1ms/step . Final metrics and graphs for all models . for n in range(len(models_to_tune)): plot_CM_ROC_PR(x_train,y_train,best_models[models_to_tune[n]],models_to_tune[n]) plot_CM_ROC_PR_NN(x_train_nn,y_train,my_model,&#39;NN&#39;) . 28/28 [==============================] - 0s 621us/step . . results_df = pd.DataFrame(results_all,columns=[&#39;model&#39;,&#39;accuracy&#39;,&#39;f1&#39;,&#39;precision&#39;,&#39;recall&#39;,&#39;auc roc&#39;,&#39;auc pr&#39;]) results_df . model accuracy f1 precision recall auc roc auc pr . 0 MNB | 0.794613 | 0.725637 | 0.744615 | 0.707602 | 0.850076 | 0.810483 | . 1 LogisticRegression | 0.830527 | 0.772247 | 0.797508 | 0.748538 | 0.874985 | 0.857189 | . 2 DT | 0.823793 | 0.763198 | 0.788162 | 0.739766 | 0.873372 | 0.820003 | . 3 RandomForest | 0.895623 | 0.855365 | 0.913621 | 0.804094 | 0.965016 | 0.950006 | . 4 Adaboost | 0.893378 | 0.857143 | 0.882353 | 0.833333 | 0.956473 | 0.941462 | . 5 GradientBoosting | 0.951740 | 0.935725 | 0.957187 | 0.915205 | 0.989287 | 0.985348 | . 6 XGBoost | 0.896745 | 0.859327 | 0.900641 | 0.821637 | 0.956612 | 0.943657 | . 7 SVM | 0.831650 | 0.772036 | 0.803797 | 0.742690 | 0.854949 | 0.829751 | . 8 NN | 0.842873 | 0.772727 | 0.868613 | 0.695906 | 0.884974 | 0.863310 | . feat_importances = best_models[&#39;XGBoost&#39;][-1].feature_importances_ features = best_models[&#39;XGBoost&#39;][:-1].get_feature_names_out() sorted_index = feat_importances.argsort() plt.barh(features[sorted_index],feat_importances[sorted_index]) . &lt;BarContainer object of 17 artists&gt; . Best results is the XGBoost model with 95% training accuracy. Most important features include title name, Pclass and Sex. . After submission to Kaggle, a score of about 75% was obtained. I can certainly do more with regards to training of hyperparameters + usage of other models. . test[&#39;Survived&#39;]=best_models[&#39;XGBoost&#39;].predict(x_test) test[[&#39;PassengerId&#39;,&#39;Survived&#39;]].to_csv(&#39;submissions.csv&#39;,index=False) .",
            "url": "https://calamin3.github.io/hello/kaggle/2022/09/05/Titanic_submission.html",
            "relUrl": "/kaggle/2022/09/05/Titanic_submission.html",
            "date": " • Sep 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Analysis on MRT/LRT dataset in Singapore",
            "content": "Introduction . In Singapore, one of the main forms of public transportation is via MRT/LRT. Adapted from an assignment, one of the ways to analyse transport patterns is the use of appropriate cluster analysis methods to group these stations into homogeneous groups using origin-destination data. The origin and destination points can be captured through the use of an electronic wallet at the gantry points of the public MRT/LRT system. LTA publishes these data (more below on how to get the data) for the public and enterprises. . We are interested to find out the weekday AM (7-9am) and PM peak (5-7pm) for the journey-to-work and journey-to-home trips, and for weekend interaction during noon times (11-1pm). . Literature . Some useful literature for reading . Public Transport OD matrices | Identifying Functional Regions in Australia Using Hierarchical Aggregation Techniques | . Getting the data . Data can be found via the Land Transport Authority Data Mall via an API call. I have also included the codes for API call but the accountkey have to be requested from LTA. . import pandas as pd import numpy as np import requests import seaborn as sns import matplotlib.pyplot as plt from pandas_profiling import ProfileReport . # import requests # def get_data(): # url = &#39;http://datamall2.mytransport.sg/ltaodataservice/PV/ODTrain&#39; # headers = {&#39;AccountKey&#39;:&#39;your key here&#39;,&#39;accept&#39;:&#39;application/json&#39;} # payload = {&#39;Date&#39;:YYYYMM} # return requests.get(url,params=payload,headers=headers).json() # link = get_data()[&#39;value&#39;][0][&#39;Link&#39;] # # extract csv from link for the month&#39;s data . df = pd.read_csv(&#39;origin_destination_train_202205.csv&#39;) df = df.drop([&#39;YEAR_MONTH&#39;,&#39;PT_TYPE&#39;],axis=1) . Displaying MRT stations and lines . For the purpose of visual displays, we will make use of the python package Folium to create a nice display of all the train stations. The coordinates of MRT stations as of now can be found pretty much anywhere, but i have used this repo and added new stations manually. . . Note: Folium does not work with fastpages for now. will look into using plotly or altair as a workaround. Have included the png image for now for all folium plots. . import folium from folium.plugins import MarkerCluster . mrt = pd.read_csv(&#39;mrt.csv&#39;) def add_markers(stations,map,colour=&#39;default&#39;): for stn in stations: stn_coor = list(mrt.loc[mrt[&#39;STN_NO&#39;]==stn,[&#39;Latitude&#39;,&#39;Longitude&#39;]].values[0]) if colour==&#39;default&#39;: stn_color = mrt.loc[mrt[&#39;STN_NO&#39;]==stn,[&#39;COLOR&#39;]].values[0][0].lower() else: stn_color = colour stn_full_name = mrt.loc[mrt[&#39;STN_NO&#39;]==stn,[&#39;STN_NAME&#39;]].values[0][0] if (ind1 := stn_full_name.find(&#39;MRT&#39;)) &gt; -1: stn_name = stn_full_name[:ind1-1].title() elif (ind2 := stn_full_name.find(&#39;LRT&#39;)) &gt; -1: stn_name = stn_full_name[:ind2-1].title() else: stn_name = stn_full_name if stn_color == &#39;gray&#39;: stn_color = &#39;lightgray&#39; #print(stn_coor,stn_color,stn) folium.Marker(location=stn_coor, popup = stn+&#39; &#39;+stn_name,icon=folium.Icon(color=stn_color)).add_to(map) . #Define coordinates of where we want to center our map sg = [1.3561519863229077, 103.80586415619761] #Create the map my_map = folium.Map(location = sg, zoom_start = 11) #add markers add_markers(mrt.STN_NO.to_list(),my_map,colour=&#39;default&#39;) # #Display the map my_map . Make this Notebook Trusted to load map: File -&gt; Trust Notebook . Let&#39;s try to visualize the MRT systems maps by colour. Since folium doesn&#39;t have the color brown, I have used pink instead for the TEL line . . Exploratory Data Analysis . A useful package for basic EDA is through Pandas Profiler. You may visit their project website for more details/ . file = ProfileReport(df) file.to_notebook_iframe() . . Weekday number of total trips are higher than that of weekends. Mean trips for all stations are higher at 93 vs 50 for weekends. Peak OD is also highest for weekdays vs weekends . df.groupby(&#39;DAY_TYPE&#39;).describe()[&#39;TOTAL_TRIPS&#39;] . count mean std min 25% 50% 75% max . DAY_TYPE . WEEKDAY 363723.0 | 93.888036 | 303.988433 | 1.0 | 4.0 | 15.0 | 59.0 | 12798.0 | . WEEKENDS/HOLIDAY 336106.0 | 50.504823 | 159.810852 | 1.0 | 3.0 | 9.0 | 32.0 | 5442.0 | . Let&#39;s try to find out the total trips by time for all stations for all days . df.groupby(&#39;TIME_PER_HOUR&#39;).describe() . TOTAL_TRIPS . count mean std min 25% 50% 75% max . TIME_PER_HOUR . 0 3884.0 | 9.386972 | 20.068909 | 1.0 | 1.0 | 3.0 | 8.0 | 230.0 | . 5 19572.0 | 27.882485 | 78.796186 | 1.0 | 2.0 | 6.0 | 21.0 | 2067.0 | . 6 32611.0 | 73.470087 | 244.369474 | 1.0 | 3.0 | 13.0 | 49.0 | 7058.0 | . 7 37441.0 | 108.842739 | 372.845040 | 1.0 | 4.0 | 17.0 | 66.0 | 10811.0 | . 8 38314.0 | 93.533669 | 310.211684 | 1.0 | 4.0 | 15.0 | 57.0 | 8510.0 | . 9 37626.0 | 64.812018 | 188.997212 | 1.0 | 3.0 | 11.0 | 43.0 | 4904.0 | . 10 37812.0 | 58.559346 | 174.290314 | 1.0 | 3.0 | 10.0 | 37.0 | 4580.0 | . 11 38699.0 | 60.849273 | 187.334462 | 1.0 | 3.0 | 10.0 | 38.0 | 4662.0 | . 12 39760.0 | 66.604930 | 209.799559 | 1.0 | 3.0 | 11.0 | 42.0 | 5523.0 | . 13 39870.0 | 69.337848 | 219.528833 | 1.0 | 3.0 | 12.0 | 43.0 | 5585.0 | . 14 39623.0 | 65.880776 | 204.457741 | 1.0 | 3.0 | 11.0 | 42.0 | 5380.0 | . 15 39987.0 | 68.949584 | 211.852530 | 1.0 | 4.0 | 12.0 | 45.0 | 6510.0 | . 16 40995.0 | 76.218392 | 233.303667 | 1.0 | 4.0 | 14.0 | 50.0 | 8898.0 | . 17 42318.0 | 106.590434 | 333.401763 | 1.0 | 5.0 | 18.0 | 69.0 | 10503.0 | . 18 41708.0 | 120.898125 | 392.323923 | 1.0 | 5.0 | 18.0 | 73.0 | 12798.0 | . 19 39260.0 | 84.383597 | 269.718346 | 1.0 | 4.0 | 14.0 | 52.0 | 8755.0 | . 20 37155.0 | 66.658135 | 220.745056 | 1.0 | 3.0 | 11.0 | 40.0 | 7971.0 | . 21 35917.0 | 61.105020 | 204.300373 | 1.0 | 3.0 | 10.0 | 37.0 | 7423.0 | . 22 33151.0 | 43.948237 | 131.877848 | 1.0 | 2.0 | 8.0 | 29.0 | 3776.0 | . 23 24126.0 | 24.061428 | 67.571074 | 1.0 | 2.0 | 5.0 | 17.0 | 2865.0 | . Morning Peak and PM Peak have the highest mean numbers, not surprisingly. We try to separate it by weekdays and weekends to see if there is any difference . df_total_trips = df.groupby([&#39;DAY_TYPE&#39;,&#39;TIME_PER_HOUR&#39;]).describe()[&#39;TOTAL_TRIPS&#39;][[&#39;count&#39;,&#39;50%&#39;,&#39;mean&#39;]].rename(columns={&#39;50%&#39;:&#39;median&#39;}).reset_index() df_total_trips=df_total_trips.melt(id_vars=[&#39;DAY_TYPE&#39;,&#39;TIME_PER_HOUR&#39;],var_name=&#39;count_type&#39;) g = sns.catplot(data=df_total_trips,x=&#39;TIME_PER_HOUR&#39;,y=&#39;value&#39;,col=&#39;DAY_TYPE&#39;, row=&#39;count_type&#39;,kind=&#39;bar&#39;,row_order=[&#39;count&#39;,&#39;mean&#39;,&#39;median&#39;],sharey=False,color=&#39;lightblue&#39;) # for ax in g.axes: # ax.tick_params(labelbottom=True) plt.show() . . We plot the graphs of the total trips by hours for both weekday and weekends. Not surprisingly, the mean and median trips are the highest during the AM and PM peak during weekdays. For weekends, the number of trips are generally constant throughout the day, with smaller number at the beginning and end of day. . How about the total trips by stations? We can have 2 groups, by origin and by destination. First, by origin. . df.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].mean().reset_index().sort_values(&#39;TOTAL_TRIPS&#39;,ascending=False) . ORIGIN_PT_CODE TOTAL_TRIPS . 76 EW24/NS1 | 259.187846 | . 119 NS21/DT11 | 208.347993 | . 72 EW2/DT32 | 205.494127 | . 120 NS22 | 203.382958 | . 110 NS13 | 199.930361 | . ... ... | ... | . 160 TE5 | 12.613582 | . 142 PW6 | 12.354680 | . 152 SW4 | 11.462328 | . 138 PW1 | 9.931076 | . 139 PW3 | 6.302691 | . 164 rows × 2 columns . plt.figure(figsize=(25,15)) g=sns.barplot(data=df.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].mean().reset_index(),x=&#39;ORIGIN_PT_CODE&#39;,y=&#39;TOTAL_TRIPS&#39;,color=&#39;lightblue&#39;) plt.xticks(rotation=90) plt.title(&#39;Mean trips by origin&#39;) plt.show() . . For analysis of the number of trips by origin, we can clearly see there are some areas that have higher volumes, for example the top 3 volume are at EW24/NS1 (Jurong East) at 259 trips, NS21/DT11 (Newton) at 208 trips and EW2/DT32 (Tampines) at 205 trips. . df.groupby(&#39;DESTINATION_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].mean().reset_index().sort_values(&#39;TOTAL_TRIPS&#39;,ascending=False) . DESTINATION_PT_CODE TOTAL_TRIPS . 76 EW24/NS1 | 261.409655 | . 120 NS22 | 220.397902 | . 130 NS9/TE2 | 220.258299 | . 119 NS21/DT11 | 214.947722 | . 110 NS13 | 210.738939 | . ... ... | ... | . 124 NS28 | 12.187382 | . 152 SW4 | 11.871693 | . 142 PW6 | 11.335381 | . 138 PW1 | 9.417886 | . 139 PW3 | 7.210399 | . 164 rows × 2 columns . plt.figure(figsize=(25,15)) g=sns.barplot(data=df.groupby(&#39;DESTINATION_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].mean().reset_index(),x=&#39;DESTINATION_PT_CODE&#39;,y=&#39;TOTAL_TRIPS&#39;,color=&#39;lightblue&#39;) plt.xticks(rotation=90) plt.title(&#39;Mean trips by destination&#39;) plt.show() . . Similarly for number of trips by destination, the top 3 volume are at EW24/NS1 (Jurong East) at 261 trips, NS22 (Orchard) at 220 trips and NS9/TE2 (Woodlands) at 220 trips. . Weekday AM Peak (7-9am) . Preparing data into OD matrix . Segmentise into weekday morning peak, weekday evening peak, weekends peak | sum of total trips by origin and destination | standardisation of data (min-max scaler) with sum of sum of origin and destination | . weekday_am_peak = df[(df[&#39;DAY_TYPE&#39;]==&#39;WEEKDAY&#39;) &amp; (df[&#39;TIME_PER_HOUR&#39;]&lt;=9) &amp; (df[&#39;TIME_PER_HOUR&#39;]&gt;=7)].drop(columns=[&#39;DAY_TYPE&#39;]) weekday_am_peak=weekday_am_peak.groupby([&#39;ORIGIN_PT_CODE&#39;,&#39;DESTINATION_PT_CODE&#39;]).sum().reset_index().drop(columns=[&#39;TIME_PER_HOUR&#39;]) . # To transform and find out the total_trips per pair as a pct of trips by the sum of trips from each origin station weekday_am_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekday_am_peak[&#39;sum_total_trips_origin&#39;]=weekday_am_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekday_am_peak[&#39;pct_trips_sum_origin&#39;]=weekday_am_peak[&#39;TOTAL_TRIPS&#39;]/weekday_am_peak[&#39;sum_total_trips_origin&#39;] #Creating OD matrix with origins as rows and destinations as columns, values as the normalised trips by origin od_weekdayam_mat = weekday_am_peak.pivot(index=&#39;ORIGIN_PT_CODE&#39;,columns=&#39;DESTINATION_PT_CODE&#39;,values=&#39;pct_trips_sum_origin&#39;).fillna(0) od_weekdayam_mat . DESTINATION_PT_CODE BP10 BP11 BP12 BP13 BP2 BP3 BP4 BP5 BP6/DT1 BP7 ... SW6 SW7 SW8 TE1 TE3 TE4 TE5 TE6 TE7 TE8 . ORIGIN_PT_CODE . BP10 0.000000 | 0.019253 | 0.046300 | 0.019304 | 0.005552 | 0.008863 | 0.033464 | 0.042429 | 0.356390 | 0.022819 | ... | 0.0 | 0.0 | 0.000000 | 0.007793 | 0.000306 | 0.001019 | 0.000611 | 0.000509 | 0.001121 | 0.000102 | . BP11 0.011440 | 0.000000 | 0.010368 | 0.013025 | 0.006255 | 0.010711 | 0.037875 | 0.031234 | 0.404542 | 0.013239 | ... | 0.0 | 0.0 | 0.000000 | 0.014781 | 0.001371 | 0.000171 | 0.000300 | 0.000000 | 0.000086 | 0.000086 | . BP12 0.021518 | 0.008227 | 0.000000 | 0.008803 | 0.007882 | 0.011219 | 0.041252 | 0.024912 | 0.427881 | 0.008170 | ... | 0.0 | 0.0 | 0.000000 | 0.014844 | 0.000288 | 0.000000 | 0.000000 | 0.000230 | 0.000173 | 0.000058 | . BP13 0.039606 | 0.017033 | 0.045044 | 0.000000 | 0.018059 | 0.012415 | 0.047917 | 0.011492 | 0.222450 | 0.003489 | ... | 0.0 | 0.0 | 0.000000 | 0.016827 | 0.000205 | 0.000103 | 0.000000 | 0.000000 | 0.000103 | 0.000000 | . BP2 0.010395 | 0.004921 | 0.014212 | 0.005243 | 0.000000 | 0.007359 | 0.024791 | 0.018398 | 0.345690 | 0.004507 | ... | 0.0 | 0.0 | 0.000000 | 0.015730 | 0.000184 | 0.000966 | 0.000000 | 0.000092 | 0.001380 | 0.001058 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . TE4 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000907 | 0.006166 | 0.000000 | 0.000363 | 0.000000 | ... | 0.0 | 0.0 | 0.000544 | 0.106456 | 0.054044 | 0.000000 | 0.019405 | 0.052956 | 0.087777 | 0.060936 | . TE5 0.000000 | 0.000000 | 0.000098 | 0.000000 | 0.000000 | 0.000295 | 0.007864 | 0.001180 | 0.000491 | 0.000295 | ... | 0.0 | 0.0 | 0.000000 | 0.153642 | 0.026639 | 0.023690 | 0.000000 | 0.029981 | 0.053868 | 0.036371 | . TE6 0.000036 | 0.000000 | 0.000107 | 0.000071 | 0.000000 | 0.000285 | 0.006656 | 0.000356 | 0.002598 | 0.000036 | ... | 0.0 | 0.0 | 0.000036 | 0.122762 | 0.018473 | 0.013561 | 0.008578 | 0.000000 | 0.078946 | 0.040648 | . TE7 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000267 | 0.003830 | 0.000000 | 0.001514 | 0.000000 | ... | 0.0 | 0.0 | 0.000000 | 0.074826 | 0.018350 | 0.030376 | 0.019776 | 0.087386 | 0.000000 | 0.042758 | . TE8 0.000000 | 0.000000 | 0.000261 | 0.000000 | 0.000000 | 0.000000 | 0.001172 | 0.000000 | 0.001693 | 0.000000 | ... | 0.0 | 0.0 | 0.000130 | 0.071382 | 0.020060 | 0.017846 | 0.015110 | 0.065260 | 0.041032 | 0.000000 | . 164 rows × 164 columns . . import scipy.cluster.hierarchy as shc from sklearn.cluster import AgglomerativeClustering . # # hierarchical clustering and display dendogram def plot_dendrogram(data: pd.DataFrame, **kwargs): clusters = shc.linkage(data,**kwargs) plt.figure(figsize=(25,16)) plt.title(&quot;Hierarchical Clustering Dendrogram&quot;) shc.dendrogram(Z=clusters,leaf_font_size=8,labels=data.index) plt.axhline(y = 0.53, color = &#39;r&#39;, linestyle = &#39;-&#39;) plt.show() plot_dendrogram(od_weekdayam_mat,method=&#39;ward&#39;,metric=&#39;euclidean&#39;) . . Finding the optimal cluster size in agglomerative hierarchical clustering is to find out the largest horizontal space that doesn&#39;t have any vertical lines(space with the longest vertical lines), with the number of clusters being the vertical lines that cut the horizontal line. This meant that the separation between clusters are the largest. We can draw the horizontal line by eyeballing the y values. Here, the number of times that the horizontal line cuts through is 8. . As the default euclidean distance metric with ward linkages works well, we will stick to the defaults. We also mentioned the default values here for simplicity of changing the linkage and affinity in order to tweak them later if needed. . def agg_cluster_fit(data: pd.DataFrame,cluster_size: int, **kwargs): model = AgglomerativeClustering(n_clusters=cluster_size,**kwargs) #affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39; model.fit(data) data[&#39;label&#39;] = model.labels_ d = [&#39;,&#39;.join(data[data[&#39;label&#39;]==x].index) for x in range(cluster_size)] df = pd.DataFrame(index=range(1,cluster_size+1),columns=[&#39;Stations&#39;],data=d) return df weekday_am_peak_clusters = agg_cluster_fit(data=od_weekdayam_mat,cluster_size=8,affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39;) weekday_am_peak_clusters.style.set_properties(**{&#39;text-align&#39;: &#39;left&#39;}) . &nbsp; Stations . 1 NE1/CC29,NE10,NE11,NE12/CC13,NE13,NE14,NE15,NE16/STC,NE17/PTC,NE5,NE7/DT12,NE8,NE9,NS24/NE6/CC1,PE1,PE2,PE3,PE4,PE5,PE6,PE7,PW1,PW3,PW4,PW5,PW6,PW7,SE1,SE2,SE3,SE4,SE5,SW1,SW2,SW3,SW4,SW5,SW6,SW7,SW8 | . 2 CC10/DT26,CC11,CC12,CC14,CC16,CC2,CC3,CC5,CC7,CC8,CG2,DT17,DT18,EW1,EW10,EW11,EW12/DT14,EW13/NS25,EW14/NS26,EW15,EW16/NE3,EW2/DT32,EW3,EW4,EW5,EW6,EW7,EW8/CC9,EW9,NE4/DT19,NS21/DT11,NS27/CE2,NS28 | . 3 CC22/EW21,EW17,EW18,EW19,EW20,EW22,EW23,EW24/NS1,EW25,EW26,EW27,EW28,EW29,EW30,EW31,EW32,EW33,NS2,NS3,NS4/BP1,NS5 | . 4 BP10,BP11,BP12,BP13,BP2,BP3,BP4,BP5,BP7,BP8,BP9 | . 5 BP6/DT1,CE1/DT16,DT10,DT13,DT2,DT3,DT5,DT6,DT7,DT8 | . 6 CC4/DT15,CC6,CG1/DT35,DT20,DT21,DT22,DT23,DT24,DT25,DT27,DT28,DT29,DT30,DT31,DT33,DT34 | . 7 CC17/TE9,CC19/DT9,CC20,CC21,CC23,CC24,CC25,CC26,CC27,CC28,TE1,TE3,TE4,TE5,TE6,TE7,TE8 | . 8 NS10,NS11,NS12,NS13,NS14,NS15,NS16,NS17/CC15,NS18,NS19,NS20,NS22,NS23,NS7,NS8,NS9/TE2 | . Lets try to visualize the clusters on a map. Each cluster is set to a different colour so that we can identify the stations in a cluster easily. . # set a colour for each cluster weekday_am_peak_clusters[&#39;cluster_colour&#39;]=[&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;darkred&#39;, &#39;lightred&#39;, &#39;beige&#39;] #Define coordinates of where we want to center our map sg = [1.3561519863229077, 103.80586415619761] #Create the map my_map = folium.Map(location = sg, zoom_start = 12) #Load list of stations mrt = pd.read_csv(&#39;mrt.csv&#39;) #add markers for i in range(len(weekday_am_peak_clusters)): cluster_list = weekday_am_peak_clusters.iloc[i].to_list()[0].split(&#39;,&#39;) colour = weekday_am_peak_clusters.iloc[i].to_list()[1] add_markers(cluster_list,my_map,colour=colour) # #Display the map my_map . Make this Notebook Trusted to load map: File -&gt; Trust Notebook . . We can see that the clustering algorithm strongly favours MRT/LRT stations that lies on the same MRT/LRT line, and also within the same region in Singapore. . Weekday PM Peak (5-7pm) . weekday_pm_peak = df[(df[&#39;DAY_TYPE&#39;]==&#39;WEEKDAY&#39;) &amp; (df[&#39;TIME_PER_HOUR&#39;]&lt;=19) &amp; (df[&#39;TIME_PER_HOUR&#39;]&gt;=17)].drop(columns=[&#39;DAY_TYPE&#39;]) weekday_pm_peak=weekday_pm_peak.groupby([&#39;ORIGIN_PT_CODE&#39;,&#39;DESTINATION_PT_CODE&#39;]).sum().reset_index().drop(columns=[&#39;TIME_PER_HOUR&#39;]) . # To transform and find out the total_trips per pair as a pct of trips by the sum of trips from each origin station weekday_pm_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekday_pm_peak[&#39;sum_total_trips_origin&#39;]=weekday_pm_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekday_pm_peak[&#39;pct_trips_sum_origin&#39;]=weekday_pm_peak[&#39;TOTAL_TRIPS&#39;]/weekday_pm_peak[&#39;sum_total_trips_origin&#39;] #Creating OD matrix with origins as rows and destinations as columns, values as the normalised trips by origin od_weekdaypm_mat = weekday_pm_peak.pivot(index=&#39;ORIGIN_PT_CODE&#39;,columns=&#39;DESTINATION_PT_CODE&#39;,values=&#39;pct_trips_sum_origin&#39;).fillna(0) od_weekdaypm_mat . DESTINATION_PT_CODE BP10 BP11 BP12 BP13 BP2 BP3 BP4 BP5 BP6/DT1 BP7 ... SW6 SW7 SW8 TE1 TE3 TE4 TE5 TE6 TE7 TE8 . ORIGIN_PT_CODE . BP10 0.000000 | 0.047719 | 0.062879 | 0.051147 | 0.026364 | 0.034142 | 0.017137 | 0.031901 | 0.268521 | 0.052333 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000264 | 0.000264 | 0.000000 | 0.000000 | 0.000264 | 0.000000 | 0.000000 | . BP11 0.037673 | 0.000000 | 0.027200 | 0.028509 | 0.035636 | 0.028218 | 0.039127 | 0.048727 | 0.318545 | 0.021527 | ... | 0.000000 | 0.000000 | 0.000000 | 0.001018 | 0.000145 | 0.000000 | 0.000000 | 0.000145 | 0.000000 | 0.000000 | . BP12 0.031290 | 0.019317 | 0.000000 | 0.028576 | 0.026341 | 0.036398 | 0.023308 | 0.033046 | 0.381386 | 0.018199 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000160 | 0.000479 | 0.000160 | 0.000160 | 0.000958 | 0.000319 | 0.000000 | . BP13 0.083534 | 0.057993 | 0.029748 | 0.000000 | 0.043570 | 0.037560 | 0.019531 | 0.022236 | 0.233173 | 0.006611 | ... | 0.000000 | 0.000000 | 0.000000 | 0.002103 | 0.000000 | 0.000300 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . BP2 0.026615 | 0.027393 | 0.024903 | 0.018988 | 0.000000 | 0.040311 | 0.018833 | 0.032840 | 0.261323 | 0.012296 | ... | 0.000000 | 0.000000 | 0.000000 | 0.001245 | 0.000934 | 0.000156 | 0.000311 | 0.000156 | 0.000000 | 0.000467 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . TE4 0.000644 | 0.000322 | 0.000161 | 0.000000 | 0.002739 | 0.003867 | 0.001450 | 0.001208 | 0.002175 | 0.000000 | ... | 0.000000 | 0.000081 | 0.000000 | 0.011278 | 0.074996 | 0.000000 | 0.023925 | 0.045433 | 0.025858 | 0.037699 | . TE5 0.001775 | 0.000197 | 0.000197 | 0.000000 | 0.000592 | 0.001183 | 0.000197 | 0.001775 | 0.002564 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.007692 | 0.114201 | 0.042801 | 0.000000 | 0.027219 | 0.032742 | 0.114004 | . TE6 0.000093 | 0.000186 | 0.000093 | 0.000000 | 0.000371 | 0.000928 | 0.000835 | 0.001485 | 0.004175 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000093 | 0.012247 | 0.086380 | 0.030803 | 0.035071 | 0.000000 | 0.061514 | 0.131286 | . TE7 0.000659 | 0.000366 | 0.000220 | 0.000000 | 0.000512 | 0.001610 | 0.000586 | 0.000000 | 0.007173 | 0.000000 | ... | 0.000586 | 0.000805 | 0.000000 | 0.003952 | 0.031618 | 0.034473 | 0.037034 | 0.107297 | 0.000000 | 0.088121 | . TE8 0.000000 | 0.000054 | 0.000107 | 0.000000 | 0.001180 | 0.001180 | 0.000161 | 0.000000 | 0.013141 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000161 | 0.005793 | 0.048917 | 0.028588 | 0.046181 | 0.126904 | 0.095259 | 0.000000 | . 164 rows × 164 columns . . # hierarchical clustering and display dendogram def plot_dendrogram(data: pd.DataFrame, **kwargs): clusters = shc.linkage(data,**kwargs) plt.figure(figsize=(25,16)) plt.title(&quot;Hierarchical Clustering Dendrogram&quot;) shc.dendrogram(Z=clusters,leaf_font_size=8,labels=data.index) plt.axhline(y = 0.7, color = &#39;r&#39;, linestyle = &#39;-&#39;) plt.show() plot_dendrogram(od_weekdaypm_mat,method=&#39;ward&#39;,metric=&#39;euclidean&#39;) . . Optimal cluster size is about 6 . def agg_cluster_fit(data: pd.DataFrame,cluster_size: int, **kwargs): model = AgglomerativeClustering(n_clusters=cluster_size,**kwargs) #affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39; model.fit(data) data[&#39;label&#39;] = model.labels_ d = [&#39;,&#39;.join(data[data[&#39;label&#39;]==x].index) for x in range(cluster_size)] df = pd.DataFrame(index=range(1,cluster_size+1),columns=[&#39;Stations&#39;],data=d) return df weekday_pm_peak_clusters = agg_cluster_fit(data=od_weekdaypm_mat,cluster_size=6,affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39;) weekday_pm_peak_clusters.style.set_properties(**{&#39;text-align&#39;: &#39;left&#39;}) . &nbsp; Stations . 1 CC10/DT26,CC11,CC12,CC14,CC16,CC17/TE9,CC19/DT9,CC2,CC20,CC21,CC23,CC24,CC25,CC26,CC27,CC28,CC3,CC4/DT15,CC5,CC6,CC7,CC8,CE1/DT16,DT10,DT13,DT17,DT18,DT2,DT3,DT5,DT6,DT7,DT8,EW12/DT14,EW16/NE3,NE1/CC29,NE10,NE11,NE12/CC13,NE13,NE14,NE15,NE16/STC,NE17/PTC,NE4/DT19,NE5,NE7/DT12,NE8,NE9,NS24/NE6/CC1,PW3 | . 2 CC22/EW21,EW11,EW13/NS25,EW14/NS26,EW15,EW17,EW18,EW19,EW20,EW22,EW23,EW24/NS1,EW25,EW26,EW27,EW28,EW29,EW30,EW31,EW32,EW33,NS10,NS11,NS12,NS13,NS14,NS15,NS16,NS17/CC15,NS18,NS19,NS2,NS20,NS21/DT11,NS22,NS23,NS27/CE2,NS28,NS3,NS4/BP1,NS5,NS7,NS8,NS9/TE2,TE1,TE3,TE4,TE5,TE6,TE7,TE8 | . 3 CG1/DT35,CG2,DT20,DT21,DT22,DT23,DT24,DT25,DT27,DT28,DT29,DT30,DT31,DT33,DT34,EW1,EW10,EW2/DT32,EW3,EW4,EW5,EW6,EW7,EW8/CC9,EW9 | . 4 PE1,PE2,PE3,PE4,PE5,PE6,PE7,PW1,PW4,PW5,PW6,PW7 | . 5 SE1,SE2,SE3,SE4,SE5,SW1,SW2,SW3,SW4,SW5,SW6,SW7,SW8 | . 6 BP10,BP11,BP12,BP13,BP2,BP3,BP4,BP5,BP6/DT1,BP7,BP8,BP9 | . Lets try to visualize the clusters on a map. Each cluster is set to a different colour so that we can identify the stations in a cluster easily. . # set a colour for each cluster weekday_pm_peak_clusters[&#39;cluster_colour&#39;]=[&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;darkred&#39;] #Define coordinates of where we want to center our map sg = [1.3561519863229077, 103.80586415619761] #Create the map my_map = folium.Map(location = sg, zoom_start = 12) #Load list of stations mrt = pd.read_csv(&#39;mrt.csv&#39;) #add markers for i in range(len(weekday_pm_peak_clusters)): cluster_list = weekday_pm_peak_clusters.iloc[i].to_list()[0].split(&#39;,&#39;) colour = weekday_pm_peak_clusters.iloc[i].to_list()[1] add_markers(cluster_list,my_map,colour=colour) # #Display the map my_map . Make this Notebook Trusted to load map: File -&gt; Trust Notebook . . Weekends Noon (11-1pm) . weekend_peak = df[(df[&#39;DAY_TYPE&#39;]==&#39;WEEKDAY&#39;) &amp; (df[&#39;TIME_PER_HOUR&#39;]&lt;=13) &amp; (df[&#39;TIME_PER_HOUR&#39;]&gt;=11)].drop(columns=[&#39;DAY_TYPE&#39;]) weekend_peak=weekend_peak.groupby([&#39;ORIGIN_PT_CODE&#39;,&#39;DESTINATION_PT_CODE&#39;]).sum().reset_index().drop(columns=[&#39;TIME_PER_HOUR&#39;]) . # To transform and find out the total_trips per pair as a pct of trips by the sum of trips from each origin station weekend_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekend_peak[&#39;sum_total_trips_origin&#39;]=weekend_peak.groupby(&#39;ORIGIN_PT_CODE&#39;)[&#39;TOTAL_TRIPS&#39;].transform(&#39;sum&#39;) weekend_peak[&#39;pct_trips_sum_origin&#39;]=weekend_peak[&#39;TOTAL_TRIPS&#39;]/weekend_peak[&#39;sum_total_trips_origin&#39;] #Creating OD matrix with origins as rows and destinations as columns, values as the normalised trips by origin od_weekend_mat = weekend_peak.pivot(index=&#39;ORIGIN_PT_CODE&#39;,columns=&#39;DESTINATION_PT_CODE&#39;,values=&#39;pct_trips_sum_origin&#39;).fillna(0) od_weekend_mat . DESTINATION_PT_CODE BP10 BP11 BP12 BP13 BP2 BP3 BP4 BP5 BP6/DT1 BP7 ... SW6 SW7 SW8 TE1 TE3 TE4 TE5 TE6 TE7 TE8 . ORIGIN_PT_CODE . BP10 0.000000 | 0.053183 | 0.063512 | 0.042993 | 0.017588 | 0.040620 | 0.031407 | 0.045505 | 0.380793 | 0.020519 | ... | 0.000000 | 0.000000 | 0.00000 | 0.000140 | 0.000419 | 0.000000 | 0.000000 | 0.000000 | 0.000279 | 0.000000 | . BP11 0.001004 | 0.000000 | 0.018936 | 0.029981 | 0.022809 | 0.040597 | 0.037441 | 0.044326 | 0.386028 | 0.003730 | ... | 0.000000 | 0.000000 | 0.00000 | 0.000430 | 0.000287 | 0.000000 | 0.000000 | 0.000143 | 0.000000 | 0.000143 | . BP12 0.010195 | 0.000703 | 0.000000 | 0.044176 | 0.028123 | 0.033161 | 0.029880 | 0.035036 | 0.488985 | 0.017928 | ... | 0.000000 | 0.000000 | 0.00000 | 0.001406 | 0.000117 | 0.000000 | 0.000000 | 0.000703 | 0.000000 | 0.000117 | . BP13 0.007932 | 0.004271 | 0.000305 | 0.000000 | 0.028981 | 0.067114 | 0.061318 | 0.021660 | 0.324283 | 0.009457 | ... | 0.000000 | 0.000000 | 0.00000 | 0.001220 | 0.000305 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . BP2 0.021167 | 0.016517 | 0.004490 | 0.004811 | 0.000000 | 0.044419 | 0.023573 | 0.024375 | 0.314144 | 0.015555 | ... | 0.000000 | 0.000000 | 0.00016 | 0.001443 | 0.000481 | 0.000000 | 0.000000 | 0.000160 | 0.000000 | 0.000481 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . TE4 0.000238 | 0.000475 | 0.000000 | 0.000000 | 0.000475 | 0.000713 | 0.001900 | 0.000000 | 0.001663 | 0.000000 | ... | 0.000000 | 0.000238 | 0.00000 | 0.020190 | 0.079335 | 0.000000 | 0.046793 | 0.062470 | 0.057957 | 0.087173 | . TE5 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000759 | 0.004556 | 0.000000 | 0.000759 | 0.000000 | ... | 0.000000 | 0.000000 | 0.00000 | 0.020121 | 0.040243 | 0.061503 | 0.000000 | 0.050114 | 0.064920 | 0.211086 | . TE6 0.000000 | 0.000000 | 0.000173 | 0.000086 | 0.000000 | 0.000778 | 0.003198 | 0.000000 | 0.002938 | 0.000086 | ... | 0.000086 | 0.000000 | 0.00000 | 0.015124 | 0.033273 | 0.029384 | 0.043039 | 0.000000 | 0.117103 | 0.133005 | . TE7 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000750 | 0.001124 | 0.000000 | 0.004497 | 0.000250 | ... | 0.000250 | 0.000125 | 0.00000 | 0.003623 | 0.026608 | 0.048595 | 0.060837 | 0.186633 | 0.000000 | 0.160400 | . TE8 0.000000 | 0.000000 | 0.000128 | 0.000000 | 0.000000 | 0.000255 | 0.000000 | 0.000000 | 0.006384 | 0.000000 | ... | 0.000128 | 0.000128 | 0.00000 | 0.008427 | 0.028218 | 0.032942 | 0.060904 | 0.114785 | 0.145940 | 0.000000 | . 164 rows × 164 columns . . # hierarchical clustering and display dendogram def plot_dendrogram(data: pd.DataFrame, **kwargs): clusters = shc.linkage(data,**kwargs) plt.figure(figsize=(25,16)) plt.title(&quot;Hierarchical Clustering Dendrogram&quot;) shc.dendrogram(Z=clusters,leaf_font_size=8,labels=data.index) plt.axhline(y = 0.82, color = &#39;r&#39;, linestyle = &#39;-&#39;) plt.show() plot_dendrogram(od_weekend_mat,method=&#39;ward&#39;,metric=&#39;euclidean&#39;) . . Optimal cluster size is about 7 . def agg_cluster_fit(data: pd.DataFrame,cluster_size: int, **kwargs): model = AgglomerativeClustering(n_clusters=cluster_size,**kwargs) #affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39; model.fit(data) data[&#39;label&#39;] = model.labels_ d = [&#39;,&#39;.join(data[data[&#39;label&#39;]==x].index) for x in range(cluster_size)] df = pd.DataFrame(index=range(1,cluster_size+1),columns=[&#39;Stations&#39;],data=d) return df weekend_peak_clusters = agg_cluster_fit(data=od_weekend_mat,cluster_size=7,affinity=&#39;euclidean&#39;, linkage=&#39;ward&#39;) weekend_peak_clusters.style.set_properties(**{&#39;text-align&#39;: &#39;left&#39;}) . &nbsp; Stations . 1 BP6/DT1,CC10/DT26,CC11,CC12,CC14,CC16,CC17/TE9,CC19/DT9,CC2,CC20,CC21,CC22/EW21,CC23,CC24,CC25,CC26,CC27,CC28,CC3,CC4/DT15,CC5,CC6,CC7,CC8,CG2,DT10,DT13,DT3,DT5,DT6,DT7,DT8,EW10,EW11,EW12/DT14,EW13/NS25,EW14/NS26,EW15,EW16/NE3,EW2/DT32,EW4,EW5,EW6,EW7,EW8/CC9,EW9,NE1/CC29,NE10,NE11,NE12/CC13,NE13,NE14,NE15,NE16/STC,NE17/PTC,NE4/DT19,NE5,NE7/DT12,NE8,NE9,NS10,NS11,NS12,NS13,NS14,NS15,NS16,NS17/CC15,NS18,NS19,NS20,NS21/DT11,NS22,NS23,NS24/NE6/CC1,NS27/CE2,NS28,NS7,NS8,NS9/TE2 | . 2 BP10,BP11,BP12,BP13,BP2,BP3,BP4,BP5,BP7,BP8,BP9,DT2 | . 3 CE1/DT16,CG1/DT35,DT17,DT18,DT20,DT21,DT22,DT23,DT24,DT25,DT27,DT28,DT29,DT30,DT31,DT33,DT34,EW1,EW3 | . 4 SE1,SE2,SE3,SE4,SE5,SW1,SW2,SW3,SW4,SW5,SW6,SW7,SW8 | . 5 TE1,TE3,TE4,TE5,TE6,TE7,TE8 | . 6 PE1,PE2,PE3,PE4,PE5,PE6,PE7,PW1,PW3,PW4,PW5,PW6,PW7 | . 7 EW17,EW18,EW19,EW20,EW22,EW23,EW24/NS1,EW25,EW26,EW27,EW28,EW29,EW30,EW31,EW32,EW33,NS2,NS3,NS4/BP1,NS5 | . Lets try to visualize the clusters on a map. Each cluster is set to a different colour so that we can identify the stations in a cluster easily. . # set a colour for each cluster weekend_peak_clusters[&#39;cluster_colour&#39;]=[&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;purple&#39;, &#39;orange&#39;, &#39;darkred&#39;, &#39;lightred&#39;] #Define coordinates of where we want to center our map sg = [1.3561519863229077, 103.80586415619761] #Create the map my_map = folium.Map(location = sg, zoom_start = 12) #Load list of stations mrt = pd.read_csv(&#39;mrt.csv&#39;) #add markers for i in range(len(weekend_peak_clusters)): cluster_list = weekend_peak_clusters.iloc[i].to_list()[0].split(&#39;,&#39;) colour = weekend_peak_clusters.iloc[i].to_list()[1] add_markers(cluster_list,my_map,colour=colour) # #Display the map my_map . Make this Notebook Trusted to load map: File -&gt; Trust Notebook . . Closing Thoughts . Since this was one of the first assignments that I have done in my data science journey, there are several items that I may could have done in addition. For example, we could have done clustering analysis for both origin and destinations to uncover the clusters in both direction. This could have revealed interesting insights on travel patterns. Also, I could have only used data from major hubs to identify where people are working, and if people choose their work location nearer to their homes and the extent of it. We could also have used PCA/SVD analysis to further identify possible number of clusters. Also, I could have applied time series analysis to identify trends within a month or across months, and especially during pre and periods of COVID-19 relaxation periods. .",
            "url": "https://calamin3.github.io/hello/sg/2022/08/21/ODMatrix.html",
            "relUrl": "/sg/2022/08/21/ODMatrix.html",
            "date": " • Aug 21, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! I am Thomas! I am a budding data science enthusiast and am interested in AI/ML, specifically towards computer vision. . You may also refer to my LinkedIn profile for more details. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://calamin3.github.io/hello/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://calamin3.github.io/hello/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}